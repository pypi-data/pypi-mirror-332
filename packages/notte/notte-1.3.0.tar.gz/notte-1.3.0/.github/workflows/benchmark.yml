name: benchmark

on:
  workflow_dispatch:
    inputs:
      agent_model:
        description: "Model agent with which to run the benchmarks"
        required: true
        default: "cerebras/llama-3.3-70b"

      n_jobs:
        description: "Number of parallel jobs to run"
        required: false
        default: "2"

      include_screenshots:
        description: "Include screenshots to llm (need compatible llm)"
        required: false
        default: false

      history_type:
        description: "How do we display history to llm"
        required: false
        default: "short_observations_with_short_data"

      tries_per_task:
        description: "How many tries per task"
        required: false
        default: "3"

concurrency:
  group: >-
    ${{ github.workflow }}-${{ github.ref }}-
    ${{ github.event.inputs.agent_model }}-
    ${{ github.event.inputs.n_jobs }}-
    ${{ github.event.inputs.include_screenshots }}-
    ${{ github.event.inputs.history_type }}-
    ${{ github.event.inputs.tries_per_task }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  CACHE_TYPE: "pip"

jobs:
  run-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4

      - name: Set environment variables
        run: |
          echo "CEREBRAS_API_KEY_CICD=${{ secrets.CEREBRAS_API_KEY_CICD }}" >> $GITHUB_ENV
          echo "OPENAI_API_KEY_CICD=${{ secrets.OPENAI_API_KEY_CICD }}" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: ${{ env.CACHE_TYPE }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit pytest mypy pytest-asyncio pytest-mock
          pip install -e .
          pip install types-requests types-beautifulsoup4 types-regex types-chevron pandas tabulate cloudpickle
          patchright install --with-deps chromium

      - name: Run benchmark unit tests
        run: |
          pytest tests/integration/test_e2e.py \
                 --capture=no \
                 -p no:asyncio \
                 --agent_llm ${{ github.event.inputs.agent_model }} \
                 --n_jobs ${{ github.event.inputs.n_jobs }} \
                 --include_screenshots ${{ github.event.inputs.include_screenshots }} \
                 --history_type ${{ github.event.inputs.history_type }} \
                 --tries_per_task ${{ github.event.inputs.tries_per_task }}

      - name: Upload md results as step summary
        if: always()
        run: cat dist/results.html >> $GITHUB_STEP_SUMMARY

      - name: Upload Logs / Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            dist/*.txt
            dist/results.html
            dist/results.jsonl
