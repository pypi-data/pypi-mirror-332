{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c2e3fd",
   "metadata": {},
   "source": [
    "# Using arcticdb to read/write market data with findatapy\n",
    "\n",
    "Apr 2024 - Saeed Amen - https://www.cuemacro.com - saeed@cuemacro.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4090bfd-0ec0-4824-9630-aae001b9bb0f",
   "metadata": {},
   "source": [
    "## What is findatapy\n",
    "\n",
    "I've been working on findatapy (and it's associated libraries) and open sourced it nearly 10 years. The basic idea of findatapy is that it can be used to download market/economic data from many sources using the same interface, and there's also ways to store this data using Parquet files and now, with ArcticDB, which I'll talk about shortly. I use findatapy regularly both in my teaching and we also use it a lot for downloading data at Turnleaf Analytics (https://www.turnleafanalytics.com) where we forecast macro economic data, alongside many proprietary libraries. As a result, it's been maintained quite regularly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db9fb4-7e46-4da2-ab01-3c7fa709107e",
   "metadata": {},
   "source": [
    "## What is ArcticDB?\n",
    "\n",
    "ArcticDB is a serverless database engine developed by the quant hedge fund Man-AHL which makes it easy to store Pandas DataFrames, which replaces Arctic (which used a MongoDB backend). What are the main reasons to use ArcticDB:\n",
    "\n",
    "* Fast\n",
    "    * It is super fast and can process millions of rows a second on disk\n",
    "* Flexible\n",
    "    * You don't need to specify a schema to start, and it also supports versioning (ie. it is bitemporal), so you can see different vintages of the same data.\n",
    "    * This is particularly useful when it comes to storing point-in-time data which is frequently revised (eg. macroeconomic data)\n",
    "    * It supports many different disk storage backends including `lmdb` (local disk backend), `mem` (in memory mostly for testing) and various buckets (including `s3`, `azure` etc.)\n",
    "* Familiar\n",
    "    * If you already know Python and Pandas then it's fairly straightforward\n",
    "\n",
    "Note, that it isn't a transactional database, so it isn't a replacement for databases like PostgresSQL or MySQL.\n",
    "\n",
    "The full documentation for ArcticDB can be found at https://docs.arcticdb.io/latest\n",
    "\n",
    "We've added support for ArcticDB in findatapy to make it easy to download market/economic data with findatapy and then store/retrieve in ArcticDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c6581-7aad-40ce-94b8-a72c53331619",
   "metadata": {},
   "source": [
    "## Installing ArcticDB and findatapy/associated libraries\n",
    "\n",
    "If you want to try out ArcticDB, one easy way to do this is to create a new conda environment with Anaconda using the below commands in your Anaconda Prompt (note, it you don't have to have Anaconda though!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2114ba-36c0-49a8-b350-4f88f9c875c5",
   "metadata": {},
   "source": [
    "`conda create -n py310arcticdb python=3.10`\n",
    "\n",
    "`conda activate py310arcticdb`\n",
    "\n",
    "`conda install anaconda`\n",
    "\n",
    "`pip install arcticdb finmarketpy chartpy findatapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8de24b",
   "metadata": {},
   "source": [
    "## Using ArcticDB with findatapy to store tick market data from Dukascopy\n",
    "\n",
    "In this notebook I'm going to show how to use ArcticDB to easily store market data using findatapy.\n",
    "\n",
    "Let's download some tick data from `dukascopy` for USDJPY spot, which is a free data source using `findatapy`. Findatapy provides a uniform wrapper to download from many different data sources. We can predefine ticker mappings from our own nicknames for tickers to the vendor tickers. It already comes out of the box, with `dukascopy` ticker mappings predefined, but these are all customisable. Note, that we haven't used the `data_engine` property. If this isn't set, then findatapy will download from our data source directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2770075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:57:50.705402Z",
     "start_time": "2022-01-26T15:57:50.701394Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can disable the log so the output is neater\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638d0de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:58:00.825278Z",
     "start_time": "2022-01-26T15:57:51.353449Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from findatapy.market import Market, MarketDataRequest\n",
    "\n",
    "# In this case we are saving predefined tick tickers to disk, and then reading back\n",
    "from findatapy.market.ioengine import IOEngine\n",
    "\n",
    "md_request_download = MarketDataRequest(\n",
    "    start_date=\"04 Jan 2021\",\n",
    "    finish_date=\"05 Jan 2021\",\n",
    "    category=\"fx\",\n",
    "    data_source='dukascopy',\n",
    "    freq=\"tick\",\n",
    "    tickers=[\"USDJPY\"],\n",
    "    fields=[\"bid\", \"ask\", \"bidv\", \"askv\"],\n",
    "    data_engine=None\n",
    ")\n",
    "\n",
    "market = Market()\n",
    "\n",
    "df_tick = market.fetch_market(md_request=md_request_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f780804",
   "metadata": {},
   "source": [
    "Let's print the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db71367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:58:02.762013Z",
     "start_time": "2022-01-26T15:58:02.738013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  USDJPY.bid  USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                    \n",
      "2021-01-04 00:00:00.247000+00:00  103.247002  103.250000          1.0   \n",
      "2021-01-04 00:00:00.349000+00:00  103.247002  103.249001          1.0   \n",
      "2021-01-04 00:00:00.715000+00:00  103.246002  103.250999          1.0   \n",
      "2021-01-04 00:00:00.816000+00:00  103.247002  103.249001          1.0   \n",
      "2021-01-04 00:00:00.917000+00:00  103.247002  103.250000          1.0   \n",
      "...                                      ...         ...          ...   \n",
      "2021-01-04 23:59:51.574000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:56.131000+00:00  103.135002  103.136002          1.0   \n",
      "2021-01-04 23:59:57.569000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:57.771000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:59.314000+00:00  103.135002  103.138000          1.0   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 00:00:00.247000+00:00         3.32  \n",
      "2021-01-04 00:00:00.349000+00:00         1.35  \n",
      "2021-01-04 00:00:00.715000+00:00         3.35  \n",
      "2021-01-04 00:00:00.816000+00:00         1.35  \n",
      "2021-01-04 00:00:00.917000+00:00         2.15  \n",
      "...                                       ...  \n",
      "2021-01-04 23:59:51.574000+00:00         3.82  \n",
      "2021-01-04 23:59:56.131000+00:00         1.25  \n",
      "2021-01-04 23:59:57.569000+00:00         4.20  \n",
      "2021-01-04 23:59:57.771000+00:00         4.01  \n",
      "2021-01-04 23:59:59.314000+00:00         3.07  \n",
      "\n",
      "[69733 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09949b07",
   "metadata": {},
   "source": [
    "Type in our ArcticDB connection string, which you'll need to change below. Note the use of `lmdb://` means we are using the local disk to manage our ArcticDB storage. We will be using a folder called `tempdatabase` in our working directory. In practice, you can obviously set a full path. To enable `findatapy` to identify it as an `arcticdb` instance, we need to write `arcticdb:` at the start. Note, that the rest of the connection string after that is precisely what you'd input into ArcticDB if you were calling it directly. The official documentation at https://docs.arcticdb.io/latest/ explains how to construct various connection strings for ArcticDB depending on the different backends. We've also illustrated how to put together a connection string for s3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d673a5bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:58:42.105404Z",
     "start_time": "2022-01-26T15:58:42.092412Z"
    }
   },
   "outputs": [],
   "source": [
    "local_storage = True\n",
    "\n",
    "# To switch between local storage or s3, it's a matter of changing the\n",
    "# connection string (also you need to make sure your AWS S3 authentication is set etc.)\n",
    "if local_storage:\n",
    "    arcticdb_conn_str = \"arcticdb:lmdb://tempdatabase?map_size=2GB\"\n",
    "else:\n",
    "    # https://docs.arcticdb.io/latest/#s3-configuration gives more details\n",
    "    # Note we need to prefix arcticdb: to the front so findatapy\n",
    "    # knows what backend engine to use\n",
    "    region = \"eu-west-2\"\n",
    "    bucket_name = \"burger_king_whopper\" # Not sure, if this name is taken :-)\n",
    "    path_prefix = \"test\"\n",
    "\n",
    "    arcticdb_conn_str = f\"arcticdb:s3s://s3.{region}.amazonaws.com:{bucket_name}?path_prefix={path_prefix}&aws_auth=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be765052-bb6c-49d5-9cf7-6e9206119c27",
   "metadata": {},
   "source": [
    "Below we set some of the parameters for writing/appending to ArcticDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315f3e75-70ce-492d-a67a-2ea74ae50190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set various parameters to govern how we write to ArcticDB, to use\n",
    "# versioning\n",
    "arcticdb_dict = {\n",
    "    # If this is set to true removes previous versions (so we only record\n",
    "    # the final version). Not pruning versions will take more disk space.\n",
    "    \"prune_previous_versions\": False,\n",
    "\n",
    "    # Do we want to append to existing records or write\n",
    "    # If you attempt to append with an overlapping chunk, you'll\n",
    "    # get an assertion failure, \"update\" allows you to change existing data\n",
    "    \"write_style\": \"write\", # \"write\" / \"append\" / \"update\"\n",
    "\n",
    "    # If set to true will remove any existing library, before writing (careful with this!!)\n",
    "    \"force_create_library\": False,\n",
    "\n",
    "    # This enables us to take advantage of ArcticDB's filtering of columns/dates\n",
    "    # otherwise we would download the full dataset, and then filter\n",
    "    # in Pandas\n",
    "    \"allow_on_disk_filter\": True,\n",
    "\n",
    "    # You can also specify your own custom queries for ArcticDB\n",
    "    \"query_builder\": None\n",
    "}\n",
    "\n",
    "# Set the ArcticDB parameters for our MarketDataRequest\n",
    "md_request_download.arcticdb_dict = arcticdb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf7708",
   "metadata": {},
   "source": [
    "We can write our tick data DataFrame into ArcticDB. We can give it the `MarketDataRequest` we used for fetching the data, which basically creates the filename in the format of `environment.category.data_source.freq.tickers` for high frequency data or in the format of `environment.category.data_source.freq` for daily data. This will enable us to more easily fetch the data using the same `MarketDataRequest` and `Market` interface. The whole point of using findatapy is that it can store ticker mappings for us, and retrieve from many different market data sources using the same interface.\n",
    "\n",
    "In this case, the symbol/table we will use for storing in ArcticDB is listed below.\n",
    "\n",
    "* `backtest.fx.tick.dukascopy.NYC.EURUSD`\n",
    "* ie. the environment of our data is `backtest`\n",
    "* the `category` is `fx`\n",
    "* the `data_source` is `dukascopy`\n",
    "* the `freq` is `tick`\n",
    "* the `cut` (or time of close) is `NYC`\n",
    "* the `tickers` is `EURUSD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27eda80-7e93-457c-bd4e-cc1746823f9c",
   "metadata": {},
   "source": [
    "## `write` to ArcticDB\n",
    "\n",
    "The Jupyter notebook [market_data_example.ipynb](../market_data_example.ipynb) explains in more detail this ticker format and the concept of a `MarketDataRequest`. We dump it disk using the `IOEngine` class. Note that the `write_time_series_cache_to_disk` and `read_time_series_from_disk` reads/writes from ArcticDB. We need to make sure that when we're writing to disk, we have a data licence to do so (and this will clearly vary between data vendors), and in particular, that only those who read from the disk are authorised to use that data. Note, that we do not need to fill the `fname` parameter, because that will automatically get constructed from the `MarketDataRequest`. We are doing a `write` to ArcticDB is in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60d1b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:58:44.132791Z",
     "start_time": "2022-01-26T15:58:43.059459Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IOEngine().write_time_series_cache_to_disk(data_frame=df_tick, engine=arcticdb_conn_str, md_request=md_request_download)\n",
    "\n",
    "# Snap the time, so we can fetch this vintage later\n",
    "earlier_download_time = datetime.datetime.now().utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532e6ec",
   "metadata": {},
   "source": [
    "We could fetch the data directly from ArcticDB using the symbol/table name ie. `backtest.fx.dukascopy.tick.NYC.USDJPY` and `IOEngine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615c9203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:58:46.009505Z",
     "start_time": "2022-01-26T15:58:45.196807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  USDJPY.bid  USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                    \n",
      "2021-01-04 00:00:00.247000+00:00  103.247002  103.250000          1.0   \n",
      "2021-01-04 00:00:00.349000+00:00  103.247002  103.249001          1.0   \n",
      "2021-01-04 00:00:00.715000+00:00  103.246002  103.250999          1.0   \n",
      "2021-01-04 00:00:00.816000+00:00  103.247002  103.249001          1.0   \n",
      "2021-01-04 00:00:00.917000+00:00  103.247002  103.250000          1.0   \n",
      "...                                      ...         ...          ...   \n",
      "2021-01-04 23:59:51.574000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:56.131000+00:00  103.135002  103.136002          1.0   \n",
      "2021-01-04 23:59:57.569000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:57.771000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:59.314000+00:00  103.135002  103.138000          1.0   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 00:00:00.247000+00:00         3.32  \n",
      "2021-01-04 00:00:00.349000+00:00         1.35  \n",
      "2021-01-04 00:00:00.715000+00:00         3.35  \n",
      "2021-01-04 00:00:00.816000+00:00         1.35  \n",
      "2021-01-04 00:00:00.917000+00:00         2.15  \n",
      "...                                       ...  \n",
      "2021-01-04 23:59:51.574000+00:00         3.82  \n",
      "2021-01-04 23:59:56.131000+00:00         1.25  \n",
      "2021-01-04 23:59:57.569000+00:00         4.20  \n",
      "2021-01-04 23:59:57.771000+00:00         4.01  \n",
      "2021-01-04 23:59:59.314000+00:00         3.07  \n",
      "\n",
      "[69733 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "symbol = \"backtest.fx.dukascopy.tick.NYC.USDJPY\" \n",
    "df_read_tick = IOEngine().read_time_series_cache_from_disk(symbol, engine=arcticdb_conn_str)\n",
    "\n",
    "print(df_read_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1efc3",
   "metadata": {},
   "source": [
    "## `append` to ArcticDB\n",
    "\n",
    "Or we can just use the `MarketDataRequest` object we populated earlier. To make it fetch from ArcticDB instead of Dukascopy, we just need to set the `data_engine` property to give it the ArcticDB connection string. Note that we are downloading data strictly after the initial data we wrote, so it won't overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6b88e6-8eee-4967-bf83-b5ae1d451621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now download a second set of data and write it for \"append\"\n",
    "# Note: we'll get an assertion error, if we try to append before the end\n",
    "# of the existing time series on disk\n",
    "md_request_download.start_date = \"06 Jan 2021\"\n",
    "md_request_download.finish_date = \"07 Jan 2021\"\n",
    "md_request_download.arcticdb_dict[\"write_style\"] = \"append\"\n",
    "\n",
    "df_tick_later = market.fetch_market(md_request=md_request_download)\n",
    "IOEngine().write_time_series_cache_to_disk(data_frame=df_tick_later,\n",
    "                                           engine=arcticdb_conn_str,\n",
    "                                           md_request=md_request_download)\n",
    "\n",
    "# This time we use the Market wrapper to download data\n",
    "# Given we don't specify an \"as_of\" property, we'll get the later version\n",
    "later_download_time = datetime.datetime.now().utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb00f63-9f04-4fe7-ad3f-c1846ccfc2c4",
   "metadata": {},
   "source": [
    "We will now fetch back the data, using the `Market` interface. Note, that this data straddles our initial download and the second one which we appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80427b57-6e38-4e2b-bdbc-fed72ad94b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_request_local_cache = MarketDataRequest(\n",
    "    md_request=md_request_download\n",
    ")\n",
    "\n",
    "md_request_local_cache.start_date = \"04 Jan 2021 10:00\"\n",
    "md_request_local_cache.finish_date = \"06 Jan 2021 14:00\"\n",
    "md_request_local_cache.data_engine = arcticdb_conn_str\n",
    "md_request_local_cache.cache_algo = \"cache_algo_return\"\n",
    "\n",
    "df_read_tick = Market().fetch_market(md_request=md_request_local_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3344340-8dc0-4b32-b581-63ef3ab2c23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No as_of specified, so we'll get the latest write!\n",
      "                                  USDJPY.bid  USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                    \n",
      "2021-01-04 10:00:00.092000+00:00  102.742996  102.745003         1.00   \n",
      "2021-01-04 10:00:00.198000+00:00  102.741997  102.745003         1.50   \n",
      "2021-01-04 10:00:00.403000+00:00  102.742996  102.746002         1.00   \n",
      "2021-01-04 10:00:00.966000+00:00  102.744003  102.747002         1.00   \n",
      "2021-01-04 10:00:01.220000+00:00  102.746002  102.748001         1.00   \n",
      "...                                      ...         ...          ...   \n",
      "2021-01-06 13:59:59.100000+00:00  103.176003  103.178001         1.31   \n",
      "2021-01-06 13:59:59.252000+00:00  103.174004  103.178001         4.87   \n",
      "2021-01-06 13:59:59.404000+00:00  103.174004  103.177002         1.87   \n",
      "2021-01-06 13:59:59.860000+00:00  103.175003  103.178001         1.50   \n",
      "2021-01-06 13:59:59.961000+00:00  103.174004  103.177002         4.62   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 10:00:00.092000+00:00         1.25  \n",
      "2021-01-04 10:00:00.198000+00:00         1.06  \n",
      "2021-01-04 10:00:00.403000+00:00         1.12  \n",
      "2021-01-04 10:00:00.966000+00:00         3.31  \n",
      "2021-01-04 10:00:01.220000+00:00         2.56  \n",
      "...                                       ...  \n",
      "2021-01-06 13:59:59.100000+00:00         1.00  \n",
      "2021-01-06 13:59:59.252000+00:00         1.69  \n",
      "2021-01-06 13:59:59.404000+00:00         1.69  \n",
      "2021-01-06 13:59:59.860000+00:00         1.50  \n",
      "2021-01-06 13:59:59.961000+00:00         1.00  \n",
      "\n",
      "[96784 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    " # We should see the 1st write and 2nd append combined, ie. latest write\n",
    "print(\"No as_of specified, so we'll get the latest write!\")\n",
    "print(df_read_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f2c22-c64e-4051-a963-e28a7c6b0b22",
   "metadata": {},
   "source": [
    "## Using `as_of` parameter to fetch different vintages\n",
    "\n",
    "We can also try reading from our earlier vintage, by setting the `as_of` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af923bc8-d2a3-44f8-bf57-a0d8399b5d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the earlier vintage write!\n",
      "                                  USDJPY.bid  USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                    \n",
      "2021-01-04 10:00:00.092000+00:00  102.742996  102.745003          1.0   \n",
      "2021-01-04 10:00:00.198000+00:00  102.741997  102.745003          1.5   \n",
      "2021-01-04 10:00:00.403000+00:00  102.742996  102.746002          1.0   \n",
      "2021-01-04 10:00:00.966000+00:00  102.744003  102.747002          1.0   \n",
      "2021-01-04 10:00:01.220000+00:00  102.746002  102.748001          1.0   \n",
      "...                                      ...         ...          ...   \n",
      "2021-01-04 23:59:51.574000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:56.131000+00:00  103.135002  103.136002          1.0   \n",
      "2021-01-04 23:59:57.569000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:57.771000+00:00  103.135002  103.138000          1.0   \n",
      "2021-01-04 23:59:59.314000+00:00  103.135002  103.138000          1.0   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 10:00:00.092000+00:00         1.25  \n",
      "2021-01-04 10:00:00.198000+00:00         1.06  \n",
      "2021-01-04 10:00:00.403000+00:00         1.12  \n",
      "2021-01-04 10:00:00.966000+00:00         3.31  \n",
      "2021-01-04 10:00:01.220000+00:00         2.56  \n",
      "...                                       ...  \n",
      "2021-01-04 23:59:51.574000+00:00         3.82  \n",
      "2021-01-04 23:59:56.131000+00:00         1.25  \n",
      "2021-01-04 23:59:57.569000+00:00         4.20  \n",
      "2021-01-04 23:59:57.771000+00:00         4.01  \n",
      "2021-01-04 23:59:59.314000+00:00         3.07  \n",
      "\n",
      "[42771 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Let's instead take the first vintage\n",
    "md_request_local_cache.as_of = earlier_download_time\n",
    "df_read_tick = Market().fetch_market(md_request=md_request_local_cache)\n",
    "\n",
    "# We should only see the earlier vintage\n",
    "print(\"See the earlier vintage write!\")\n",
    "print(df_read_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ff418-f579-4ecb-963f-87de63500e77",
   "metadata": {},
   "source": [
    "We can now try to get the later vintage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f6e527-1f22-42b7-b6fa-51ac25acdc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the latest vintage write!\n",
      "                                  USDJPY.bid  USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                    \n",
      "2021-01-04 10:00:00.092000+00:00  102.742996  102.745003         1.00   \n",
      "2021-01-04 10:00:00.198000+00:00  102.741997  102.745003         1.50   \n",
      "2021-01-04 10:00:00.403000+00:00  102.742996  102.746002         1.00   \n",
      "2021-01-04 10:00:00.966000+00:00  102.744003  102.747002         1.00   \n",
      "2021-01-04 10:00:01.220000+00:00  102.746002  102.748001         1.00   \n",
      "...                                      ...         ...          ...   \n",
      "2021-01-06 13:59:59.100000+00:00  103.176003  103.178001         1.31   \n",
      "2021-01-06 13:59:59.252000+00:00  103.174004  103.178001         4.87   \n",
      "2021-01-06 13:59:59.404000+00:00  103.174004  103.177002         1.87   \n",
      "2021-01-06 13:59:59.860000+00:00  103.175003  103.178001         1.50   \n",
      "2021-01-06 13:59:59.961000+00:00  103.174004  103.177002         4.62   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 10:00:00.092000+00:00         1.25  \n",
      "2021-01-04 10:00:00.198000+00:00         1.06  \n",
      "2021-01-04 10:00:00.403000+00:00         1.12  \n",
      "2021-01-04 10:00:00.966000+00:00         3.31  \n",
      "2021-01-04 10:00:01.220000+00:00         2.56  \n",
      "...                                       ...  \n",
      "2021-01-06 13:59:59.100000+00:00         1.00  \n",
      "2021-01-06 13:59:59.252000+00:00         1.69  \n",
      "2021-01-06 13:59:59.404000+00:00         1.69  \n",
      "2021-01-06 13:59:59.860000+00:00         1.50  \n",
      "2021-01-06 13:59:59.961000+00:00         1.00  \n",
      "\n",
      "[96784 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# We can also specify the later write time\n",
    "md_request_local_cache.as_of = later_download_time\n",
    "df_read_tick = Market().fetch_market(md_request=md_request_local_cache)\n",
    "\n",
    "# We should only see the latest vintage\n",
    "print(\"See the latest vintage write!\")\n",
    "print(df_read_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6001744-c665-40ca-bc65-bfabf95c21fb",
   "metadata": {},
   "source": [
    "## `update` to ArcticDB\n",
    "\n",
    "Download date for `6 Jan 2021` and then write to disk as an `update` (after multiplying it), modifying an exisitng chunk of data already on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349a03fa-debd-4bcb-aa12-00bc631b7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's try doing an update of an existing continuous chunk\n",
    "# Note: if part of our update ends up being before/after the existing\n",
    "# dataset, it will fail\n",
    "md_request_download.start_date = \"06 Jan 2021\"\n",
    "md_request_download.finish_date = \"07 Jan 2021\"\n",
    "md_request_download.arcticdb_dict[\"write_style\"] = \"update\"\n",
    "\n",
    "df_tick_later = market.fetch_market(md_request=md_request_download)\n",
    "\n",
    "# Modify the data, so we can see the obvious difference, when reading back later!\n",
    "df_tick_later = df_tick_later * 10.0\n",
    "IOEngine().write_time_series_cache_to_disk(data_frame=df_tick_later,\n",
    "                                           engine=arcticdb_conn_str,\n",
    "                                           md_request=md_request_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f82b21-db70-4080-815a-ec09d663c124",
   "metadata": {},
   "source": [
    "Let's read back from an earlier chunk, including the updating section. Given we don't specify the `as_of` it will just give us the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdd4214c-696d-4a75-a43c-a17cb68d5d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tick (should be 10 larger!)\n",
      "                                   USDJPY.bid   USDJPY.ask  USDJPY.bidv  \\\n",
      "Date                                                                      \n",
      "2021-01-04 10:00:00.092000+00:00   102.742996   102.745003          1.0   \n",
      "2021-01-04 10:00:00.198000+00:00   102.741997   102.745003          1.5   \n",
      "2021-01-04 10:00:00.403000+00:00   102.742996   102.746002          1.0   \n",
      "2021-01-04 10:00:00.966000+00:00   102.744003   102.747002          1.0   \n",
      "2021-01-04 10:00:01.220000+00:00   102.746002   102.748001          1.0   \n",
      "...                                       ...          ...          ...   \n",
      "2021-01-06 23:59:40.194000+00:00  1030.250000  1030.280029         12.5   \n",
      "2021-01-06 23:59:40.295000+00:00  1030.250000  1030.270020         10.0   \n",
      "2021-01-06 23:59:40.598000+00:00  1030.250000  1030.270020         10.0   \n",
      "2021-01-06 23:59:40.700000+00:00  1030.219971  1030.270020         10.0   \n",
      "2021-01-06 23:59:44.474000+00:00  1030.229980  1030.260010         10.0   \n",
      "\n",
      "                                  USDJPY.askv  \n",
      "Date                                           \n",
      "2021-01-04 10:00:00.092000+00:00     1.250000  \n",
      "2021-01-04 10:00:00.198000+00:00     1.060000  \n",
      "2021-01-04 10:00:00.403000+00:00     1.120000  \n",
      "2021-01-04 10:00:00.966000+00:00     3.310000  \n",
      "2021-01-04 10:00:01.220000+00:00     2.560000  \n",
      "...                                       ...  \n",
      "2021-01-06 23:59:40.194000+00:00    13.900000  \n",
      "2021-01-06 23:59:40.295000+00:00    24.400002  \n",
      "2021-01-06 23:59:40.598000+00:00    29.500000  \n",
      "2021-01-06 23:59:40.700000+00:00    28.900002  \n",
      "2021-01-06 23:59:44.474000+00:00    37.000000  \n",
      "\n",
      "[131522 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "md_request_local_cache.start_date = \"04 Jan 2021 10:00\"\n",
    "md_request_local_cache.finish_date = \"08 Jan 2021 14:00\"\n",
    "md_request_local_cache.as_of = None\n",
    "df_read_updated_tick = Market().fetch_market(md_request=md_request_local_cache)\n",
    "\n",
    "print(\"Updated tick (should be 10 larger!)\")\n",
    "print(df_read_updated_tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc582c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f30215",
   "metadata": {},
   "source": [
    "We have seen how we can use findatapy combined with ArcticDB, to download market data pretty easy and also how to write/read this same data from ArcticDB. We have seen the different types of writing, notable `write`, `append` and `update`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
