{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tutorial for Implicit Deep Learning\n",
    "\n",
    "This tutorial introduces the **Implicit Deep Learning** (IDL) framework using the `idl` package in 3 main parts:\n",
    "\n",
    "1. **A Simple Example**\n",
    "    - Implicit Model\n",
    "    - Implcit RNN\n",
    "    - State-driven Implicit Model (SIM)\n",
    "3. **Custom Activation for Implicit model**\n",
    "4. **Implicit model as a layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Simple Example\n",
    "\n",
    "This section provides a quick guide on how to use our package. With just a few lines of code, you can get started effortlessly.\n",
    "\n",
    "Before proceeding, please ensure you have installed the required packages by following the [installation](https://github.com/HoangP8/Implicit-Deep-Learning?tab=readme-ov-file#installation) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. `ImplicitModel`\n",
    "\n",
    "`ImplicitModel` is the most fundamental implicit model. Unlike traditional architectures, it solves an fixed-point equation to find hidden states. For details on its parameters and the underlying intuition, please refer to the [documentation](https://implicit-deep-learning.readthedocs.io/en/latest/api/idl.html).\n",
    "\n",
    "In this example, we demonstrate how to use the model for a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5919\n",
      "Epoch [2/10], Loss: 1.0334\n",
      "Epoch [3/10], Loss: 0.4830\n",
      "Epoch [4/10], Loss: 0.1951\n",
      "Epoch [5/10], Loss: 0.1479\n",
      "Epoch [6/10], Loss: 0.1692\n",
      "Epoch [7/10], Loss: 0.1399\n",
      "Epoch [8/10], Loss: 0.0868\n",
      "Epoch [9/10], Loss: 0.0465\n",
      "Epoch [10/10], Loss: 0.0318\n",
      "Inference result: \n",
      " tensor([[-0.0525,  0.5056, -0.1804, -0.2234, -0.2438, -0.4717, -0.2398, -0.4559,\n",
      "          0.0045, -0.1295]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random input and output data\n",
    "x = torch.randn(5, 64).to(device)  # (batch_size=5, input_dim=64)\n",
    "y = torch.randn(5, 10).to(device)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64).to(device)\n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImplicitModel` has its forward and backward passes **fully packaged**, ensuring that the training and inference steps work **as normal**, with no additional modifications required. You only need to define the model with the appropriate `input_dim`, `output_dim`, and `hidden_dim`, and use it just like any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. `ImplicitRNN`\n",
    "\n",
    "`ImplicitRNN` uses an implicit layer to define recurrence within a standard RNN framework. For more details, please refer to the [documentation](https://implicit-deep-learning.readthedocs.io/en/latest/api/rnn.html).\n",
    "\n",
    "Its usage is very similar to `ImplicitModel`. Below, we provide an example where the model learns to predict a single output from an input sequence in a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8179\n",
      "Epoch [2/10], Loss: 0.8017\n",
      "Epoch [3/10], Loss: 0.7861\n",
      "Epoch [4/10], Loss: 0.7708\n",
      "Epoch [5/10], Loss: 0.7557\n",
      "Epoch [6/10], Loss: 0.7392\n",
      "Epoch [7/10], Loss: 0.7199\n",
      "Epoch [8/10], Loss: 0.6989\n",
      "Epoch [9/10], Loss: 0.6883\n",
      "Epoch [10/10], Loss: 0.6879\n",
      "Inference result: tensor([[-0.5798]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitRNN\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random input and output sequence \n",
    "x = torch.randn(50, 20, 1).to(device)  # (batch_size=50, seq_len=20, input_dim=1)\n",
    "y = torch.randn(50, 1).to(device)  # (batch_size=50, output_dim=1)\n",
    "\n",
    "# Initialize the ImplicitRNN model\n",
    "model = ImplicitRNN(input_dim=1, output_dim=1, hidden_dim=10, implicit_hidden_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loops\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Inference step\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = torch.randn(1, 20, 1).to(device)\n",
    "    y_pred = model(x_test)\n",
    "    print(f\"Inference result: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. `SIM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SIM` (State-driven Implicit Modeling) is a training method that helps implicit models learn from pre-trained explicit networks by matching their internal state vectors. For more details, please refer to the [documentation](https://implicit-deep-learning.readthedocs.io/en/latest/api/sim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# First define a simple feed forward network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 4:\n",
    "            x = x.squeeze(1).flatten(start_dim=-2)\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "    def scale_network(self, factor=0.9):\n",
    "        \"\"\"\n",
    "        Scale all the weights of the network by the maximum norm so that there exist a solution to the SIM convex optimization problem.\n",
    "        \"\"\"\n",
    "        layers_indices = [0, 2, 4]\n",
    "\n",
    "        max_norm = max(\n",
    "            torch.linalg.norm(self.linear_relu_stack[i].weight, np.inf)\n",
    "            for i in layers_indices\n",
    "        )\n",
    "\n",
    "        for i in layers_indices:\n",
    "            weight = self.linear_relu_stack[i].weight\n",
    "            scaled_weight = torch.nn.Parameter(weight / (max_norm * factor))\n",
    "            self.linear_relu_stack[i].weight = scaled_weight\n",
    "\n",
    "        scaled_norm = max(\n",
    "            torch.linalg.norm(self.linear_relu_stack[i].weight, np.inf)\n",
    "            for i in layers_indices\n",
    "        )\n",
    "        print(f\"Original norm : {max_norm}, Scaled norm: {scaled_norm}\")\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch 1: Average loss: 0.5134, Accuracy: 81.49%\n",
      "Test Epoch 2: Average loss: 0.4346, Accuracy: 84.36%\n",
      "Test Epoch 3: Average loss: 0.4271, Accuracy: 84.78%\n",
      "Test Epoch 4: Average loss: 0.4380, Accuracy: 84.19%\n",
      "Test Epoch 5: Average loss: 0.4171, Accuracy: 85.36%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 128\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data', train=True, download=True, transform=transform),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./data', train=False, download=True, transform=transform),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = Model(input_size=784, output_size=10).to(device)\n",
    "\n",
    "# Define optimizer, loss function, and lr scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0.0001)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    # if epoch == EPOCHS - 1:\n",
    "    #     # Scale the network to ensure there exist a solution to the SIM convex optimization problem\n",
    "    #     model = model.scale_network(0.9)\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct += torch.sum(preds == target.data).item()\n",
    "        test_loss = (test_loss / len(test_loader.dataset))\n",
    "        accuracy = correct / len(test_loader.dataset) * 100\n",
    "        print(f'Test Epoch {epoch+1}: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:45<00:00, 101.25s/it]\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuruacy: 0.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(82.52000000000001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from idl.sim import SIM\n",
    "from idl.sim.solvers import CVXSolver\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Take only a subset of the training dataset to train the state-driven model\n",
    "selected_indices = random.sample(\n",
    "    range(len(train_loader.dataset)), 2000\n",
    ")\n",
    "subset = Subset(train_loader.dataset, selected_indices)\n",
    "subset_loader = DataLoader(subset, batch_size=1000, shuffle=True)\n",
    "\n",
    "sim = SIM(activation_fn=torch.nn.functional.relu, device=device, dtype=torch.float32)\n",
    "\n",
    "solver = CVXSolver(regen_states=True)\n",
    "\n",
    "# Train SIM\n",
    "sim.train(solver=solver, model=model, dataloader=subset_loader)\n",
    "\n",
    "# Evaluate SIM\n",
    "sim.evaluate(test_loader) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Activation for Implicit model\n",
    "The default activation of the Implicit model is ReLU. To override the implicit function you wish to use, just simply replace the `phi` and `dphi` (gradient of activation) methods. Below is an example of SiLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplicitFunctionInf: function to ensure wellposedness of Implicit model\n",
    "from idl import ImplicitModel, ImplicitFunctionInf \n",
    "import torch\n",
    "\n",
    "class ImplicitFunctionInfSiLU(ImplicitFunctionInf):\n",
    "    \"\"\"\n",
    "    An implicit function that uses the SiLU nonlinearity.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(X):\n",
    "        return X * torch.sigmoid(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def dphi(X):\n",
    "        grad = X.clone().detach()\n",
    "        sigmoid = torch.sigmoid(grad)\n",
    "        return sigmoid * (1 + grad * (1 - sigmoid))\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ImplicitModel(input_dim=64,\n",
    "                      output_dim=10, \n",
    "                      hidden_dim=128,\n",
    "                      f=ImplicitFunctionInfSiLU)\n",
    "\n",
    "# train model normally after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit model as a layer\n",
    "Implicit Model can be integrated as a layer within larger models, allowing it to be trained as part of the overall network. The training process works normally, below is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7000\n",
      "Epoch [2/10], Loss: 0.3324\n",
      "Epoch [3/10], Loss: 0.1701\n",
      "Epoch [4/10], Loss: 0.0583\n",
      "Epoch [5/10], Loss: 0.0516\n",
      "Epoch [6/10], Loss: 0.0430\n",
      "Epoch [7/10], Loss: 0.0322\n",
      "Epoch [8/10], Loss: 0.0248\n",
      "Epoch [9/10], Loss: 0.0216\n",
      "Epoch [10/10], Loss: 0.0203\n",
      "Inference result: \n",
      " tensor([[-0.7536, -0.1237,  0.1082, -0.7727,  0.6030,  1.1026,  0.3494,  0.2714,\n",
      "          0.3646,  0.4176]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from idl import ImplicitModel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a larger model that includes ImplicitModel as a layer\n",
    "class MLPWithImplicit(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, implicit_hidden_dim, output_dim):\n",
    "        super(MLPWithImplicit, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.implicit_layer = ImplicitModel(input_dim=hidden_dim, output_dim=output_dim, hidden_dim=implicit_hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.implicit_layer(x)  # Pass through ImplicitModel\n",
    "        return x\n",
    "\n",
    "# Random input and output data\n",
    "x = torch.randn(5, 64).to(device)  # (batch_size=5, input_dim=64)\n",
    "y = torch.randn(5, 10).to(device)  # (batch_size=5, output_dim=10)\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPWithImplicit(input_dim=64, hidden_dim=128, implicit_hidden_dim=64, output_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define MSE loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() \n",
    "    output = model(x)  # Forward pass\n",
    "    loss = criterion(output, y)  # Compute MSE loss\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# Inference step\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    x_test = torch.randn(1, 64).to(device)  \n",
    "    y_pred = model(x_test)  \n",
    "    print(f\"Inference result: \\n {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
