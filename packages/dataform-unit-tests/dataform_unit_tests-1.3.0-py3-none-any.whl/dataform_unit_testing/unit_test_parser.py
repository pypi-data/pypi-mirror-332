"""
Copyright 2025 Neeraj Morar

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
import re

from google.cloud import bigquery
from jsonschema import validate, ValidationError

BQ_CLIENT = bigquery.Client()

SCHEMA_SPEC = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "description": {"type": "string"},
        "model_to_test": {"type": "string"},
        "input_data": {
            "type": "object",
            "patternProperties": {
                "^[\w]+\.[\w]+$": { #key of any name
                    "type": ["array", "null"],
                    "items": {"type": "object"}
                }
            },
            "additionalProperties": False
        },
        "expected_output": {
            "type": "array",
            "items": {"type": "object"}
        }
    },
    "required": ["name", "model_to_test", "input_data", "expected_output"],
    "additionalProperties": False
}

def validate_schema(json):
    """
    Takes a JSON converted into a Python dictionary and validates against
    a defined schema spec.

    Args:
        json (dict): The JSON object which is contained as Python dict object.

    Returns:
        bool: True if it's valid against the spec; False if not.
    """
    try:
        validate(instance=json, schema=SCHEMA_SPEC)
        return True
    except ValidationError as e:
        return False


def parse_schema(schema):
    """
    Takes a Python dict generated by the BigQuery.query() client representing the schema
    of a query and converts into dot-notation format with its datatype.

    Args:
        schema (dict): The Python dict generated by the BigQuery.query() client, specifically _properties["statistics"]["query"]["schema"]["fields"]

    Returns:
        dict: Columns (in dot-notation format) are keys, and values are the datatypes (e.g. {"column_name": "STRING", "column_name.sub_column": "INTEGER"})
    """
    column_and_datatype = {}
    for column in schema:
        if column['type'] != "RECORD":
            if column['type'] == "FLOAT":
                column['type'] = "FLOAT64"
            datatype = f"ARRAY<{column['type']}>" if column["mode"] == "REPEATED" else column["type"]
            column_and_datatype[column["name"]] = datatype
        else:
            column_and_datatype[column["name"]] = column["type"]
            record_column_and_datatype = parse_schema(column["fields"])
            for col, dtype in record_column_and_datatype.items():
                col_key = f"{column['name']}.{col}"
                column_and_datatype[col_key] = dtype

    return column_and_datatype


def get_bq_table_datatypes(gcp_project_id, table_name, query = None):
    """
    Takes either a table name or a pre-defined query and generates the BigQuery schema for it.

    Args:
        gcp_project_id (string): The GCP Project ID to execute the query against
        table_name (string): The table name you want the schema for
        query (string) (Optional): The pre-defined query you want to generate a schema for

    Returns:
        dict: Columns (in dot-notation format) are keys, and values are the datatypes (e.g. {"column_name": "STRING", "column_name.sub_column": "INTEGER"})
    """
    job_config = bigquery.QueryJobConfig(dry_run=True)

    if query is None:
        query = f"SELECT * FROM `{gcp_project_id}.{table_name}`"
    
    query_job = BQ_CLIENT.query(
        query = query,
        job_config = job_config
    )

    schema = query_job._properties["statistics"]["query"]["schema"]["fields"]
    
    return parse_schema(schema)


def assign_datatype(rows, column_dtypes):
    """
    Takes a Python list of data representing rows of a dataset and assigns the BigQuery datatypes identified for those columns

    Args:
        rows (list of dicts): The Python list containing dicts, each dict representing a single row of data with keys being column names, and values being the value for that column
        column_dtypes (dict): A dictionary of column names and their BigQuery datatypes

    Returns:
        None
    """
    for row in rows:
        for col, col_val in row.items():
            row[col] = [col_val, column_dtypes[col]]
            if column_dtypes[col] == "RECORD":
                keys_list = list(column_dtypes.keys())
                record_column_dtypes = {}
                for key in keys_list:
                    if key.startswith(f"{col}."):
                        dtype = column_dtypes[key]
                        key = key.replace(f"{col}.", "")
                        record_column_dtypes[key] = dtype
                assign_datatype(col_val, record_column_dtypes)


def mock_null_rows(null_rows_obj, column_dtypes):
    """
    Evaluates if an object representing mocked rows is None, if so then create a mocked row of SQL nulls.

    Args:
        null_rows_obj (list | None): The rows object to evaluate if it's a None or a list representing a set of table rows
        column_dtypes (dict): A dict containing the column names and datatypes of the table to mock

    Returns:
        list: A list of mocked SQL nulls to their corresponding column names, or the original list if it wasn't a None type
    """
    if null_rows_obj is None:
        mocked_row = {}
        for col in column_dtypes.keys():
            mocked_row[col] = None
        return [mocked_row]
    else:
        return null_rows_obj


def scaffold_tables(gcp_project_id, tables_to_scaffold, output_query = None):
    """
    Takes a JSON object representing a BigQuery table and identifies the BigQuery datatypes automatically.

    Args:
        gcp_project_id (string): The GCP Project ID to execute the query against
        tables_to_scaffold (dict or list): The table(s) you want to identify datatypes for
        output_query (string) (Optional): The pre-defined query you want to generate a schema for

    Returns:
        None
    """
    if isinstance(tables_to_scaffold, dict):
        for table, rows in tables_to_scaffold.items():
            schema_and_table = table.split(".")
            matches = re.findall(f"\`(.+?){schema_and_table[0]}\.{schema_and_table[1]}\`", output_query, flags=re.MULTILINE)
            if len(matches) > 0:
                gcp_project_id = matches[0]
                gcp_project_id = gcp_project_id[:-1]
            column_and_datatypes = get_bq_table_datatypes(gcp_project_id, table)
            rows = mock_null_rows(rows, column_and_datatypes)
            assign_datatype(rows, column_and_datatypes)
    elif isinstance(tables_to_scaffold, list):
        column_and_datatypes = get_bq_table_datatypes(gcp_project_id, "expected_output", output_query)
        assign_datatype(tables_to_scaffold, column_and_datatypes)


def check_input_tables_exist(test, query_to_test):
    """
    Takes a JSON object representing a unit test and identifies if the provided input tables exist in the Dataform model being tested.

    Args:
        test (dict): A dict representing a unit test
        query_to_test (string): The SQL string of the Dataform model to be tested

    Returns:
        bool: True if all input tables exist in the model; False if at least one is not
    """
    try:
        for key in test["input_data"].keys():
            schema_and_table = key.split(".")
            matches = re.findall(f"\`.+?{schema_and_table[0]}\.{schema_and_table[1]}\`", query_to_test, flags=re.MULTILINE)
            if len(matches) == 0:
                return False
    except:
        return False
        
    return True


def check_input_table_cols_match(input_tables):
    """
    Takes a dict of input tables which are lists of dicts and checks if each list of dict has the same columns as each other

    Args:
        input_tables (dict): A dict where each key accesses a list of dicts representing an input table

    Returns:
        bool: True if all input tables have consistent columns; False if at least one is not
    """
    for table in input_tables.keys():
        if not is_different_columns(input_tables[table]):
            return False
        
    return True


def check_expected_output_cols_match(expected_output):
    """
    Takes a list of dicts which represents the expected output of a Dataform model and verifies each row (dict) has the columns as each other

    Args:
        expected_output (list): A list of dicts representing the expected output

    Returns:
        bool: True if all rows have consistent columns; False if not
    """
    return is_different_columns(expected_output)


def is_different_columns(table):
    col_set = {tuple(row.keys()) for row in table}
    if len(col_set) > 1:
        return False
        
    return True
