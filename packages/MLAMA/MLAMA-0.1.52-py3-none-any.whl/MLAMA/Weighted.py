# -*- coding: utf-8 -*-
"""Weighted.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YQ95wI-wzXmHgpq41Y0fTDgia0tEZq7o
"""

#test.r from the main version to python Post-hoc Optimization Code.r from the cleaner version
#weighted ARIMA used in the following parts????

#observe kivabe calculate korse?
#MAIN MLAMA Visualization here

import pandas as pd
import numpy as np
from scipy.optimize import minimize

#Weighted.py
#test.r from the main version to python Post-hoc Optimization Code.r from the cleaner version
#weighted ARIMA used in the following parts????

#obse

#generalized

from scipy.optimize import minimize


# Cost function to minimize
def cost_function(weights, predictions, observed):
    #weighted_predictions = np.dot(predictions, weights)
    #weighted_predictions = [a*b for a,b in zip(predictions, weights)]
    weighted_predictions = predictions*weights
    r = np.sum((observed - weighted_predictions) ** 2)#squared error
    # If sum of weights is not 1, apply a penalty (optional)
    #if np.sum(weights) != 1:
     #   r += 1000000000000000
    #print(r)
    return r

#generalize
def find_diff(data, models, observed_column_name='Observed'):
    """
    Generalized function to calculate differences between model predictions and observed values.

    Parameters:
    - data: DataFrame containing model predictions and observed values
    - models: Dictionary of models where keys are model names
    - observed_column_name: Column name for the observed data (default is 'Observed')

    Returns:
    - DataFrame with new columns containing the differences for each model
    """
    # Iterate through the models dictionary and calculate differences
    for model_name in models.keys():
        if model_name in data.columns:
            diff_column = f'diff_{model_name}'
            data[diff_column] = data[model_name] - data[observed_column_name]

    return data


def find_optimal_weights(data, predictions, delay_list, observed_column_name, models):
    """
    Function to find optimal weights for each unique combination of Delay and Length.

    Parameters:
        data (pd.DataFrame): The dataset containing predictions and observed values.
        predictions (list): List of prediction lengths to iterate over.
        delay_list (list): List of delay values to iterate over.
        observed_column_name (str): Column name for the observed values.
        models (dict): Dictionary of models with their names as keys and classes as values.

    Returns:
        dict: A dictionary containing the optimal weights for each delay and length combination.
    """
    optimal_weights = {}

    # Extract model names
    model_names = list(models.keys())

    # Iterate over unique combinations of Delay and Length
    for length in predictions:
        for delay in delay_list:
            # Filter the data for the specific combination of delay and length
            #print(data[(data['Delay'] == delay) & (data['Length'] == length)])
            combo_data = data[(data['Delay'] == delay) & (data['Length'] == length)]

            # Keep only numeric columns for groupby mean
            numeric_columns = combo_data.select_dtypes(include=['number']).columns
            combo_data = combo_data[numeric_columns].groupby('Wave').mean()
            # Extract the predictions for the selected models
            predicted_values = combo_data[model_names].values
            observed = combo_data[observed_column_name].values

            # Initial weights: inverse squared differences
            diff_columns = [f"diff_{model}" for model in model_names]
            x = 1 / combo_data[diff_columns].pow(2)
            initial_weights = (x.div(x.sum(axis=1), axis=0)).mean().values

            # Optimizing weights using minimize
            opt_res = minimize(
                cost_function,  # Assumes cost_function is predefined
                initial_weights,
                args=(predicted_values, observed),
                method='L-BFGS-B',
                bounds=[(0, 1)] * len(model_names)
            )

            # Storing the results
            optimal_weights[f"{delay} {length}"] = opt_res.x

    return optimal_weights



def find_stan_optimal_weights(data, predictions, delay_list, observed_column_name, models):
  optimal_weights = find_optimal_weights(data, predictions, delay_list, observed_column_name, models)
  # Standardizing the optimal weights
  stan_optimal_weights = {k: v/np.sum(v) for k, v in optimal_weights.items()}
  return stan_optimal_weights

#generalized

def find_weighted_predictions(data, stan_optimal_weights, observed_column_name, models):
    """
    Function to calculate weighted predictions and evaluate performance (MAPE) for given models and optimal weights.

    Parameters:
        data (pd.DataFrame): The dataset containing predictions, observed values, and scenario details (e.g., Delay, Length).
        stan_optimal_weights (dict): Dictionary containing optimal weights for each scenario as {scenario_name: weights}.
        observed_column_name (str): Column name for the observed values.
        models (dict): Dictionary of models with their names as keys and classes as values.

    Returns:
        theDD (pd.DataFrame): Summary of average MAPE by algorithm and adaptability (delay).
        theD_long (pd.DataFrame): Long-format DataFrame with MAPE values for visualization.
    """
    # Initialize an empty DataFrame for combined results
    combined_df = pd.DataFrame()

    # Iterate through each scenario and its weights
    for scenario_name, weights in stan_optimal_weights.items():
        delay, length = map(float, scenario_name.split())
        # Dynamically create columns for the weights based on model names
        temp_df = pd.DataFrame([weights], columns=[f"weight_{model}" for model in models.keys()])
        temp_df['Delay'] = delay
        temp_df['Length'] = length

        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)

    # Merge the weights with the original data
    joined_df = pd.merge(combined_df, data, on=['Delay', 'Length'])

    # Calculate the weighted predictions dynamically
    weighted_prediction = sum(
        joined_df[model] * joined_df[f"weight_{model}"] for model in models.keys()
    )
    joined_df['weighted_y'] = weighted_prediction

    # Calculate MAPE for each model and weighted predictions
    for model in models.keys():
        joined_df[f"{model}_MAPE"] = (
            abs(joined_df[model] - joined_df[observed_column_name]) / joined_df[observed_column_name]
        )
    joined_df['Weighted_MAPE'] = (
        abs(joined_df['weighted_y'] - joined_df[observed_column_name]) / joined_df[observed_column_name]
    )

    # Grouping by Delay, Wave, and Length, and summarizing MAPE
    mape_columns = {f"{model}_MAPE": (f"{model}_MAPE", 'mean') for model in models.keys()}
    mape_columns['Weighted_MAPE'] = ('Weighted_MAPE', 'mean')

    theD = joined_df.groupby(['Delay', 'Wave', 'Length']).agg(**mape_columns).reset_index()

    # Convert percentages
    mape_percentage_columns = [f"{model}_MAPE" for model in models.keys()] + ['Weighted_MAPE']
    theD[mape_percentage_columns] *= 100

    # Rename columns
    theD.columns = ['Adaptability', 'Wave', 'Prediction Length'] + mape_percentage_columns

    # Convert from wide to long format for visualization
    theD_long = pd.melt(
        theD,
        id_vars=["Wave", "Adaptability", "Prediction Length"],
        value_vars=mape_percentage_columns,
        var_name="Algorithm",
        value_name="MAPE"
    )

    # Remove '_MAPE' suffix from Algorithm names
    theD_long['Algorithm'] = theD_long['Algorithm'].str.replace('_MAPE', '')

    # Group data and calculate mean MAPE by algorithm and adaptability
    theDD = theD_long.groupby(['Algorithm', 'Adaptability']).agg({'MAPE': 'mean'}).reset_index()

    return theDD, theD_long