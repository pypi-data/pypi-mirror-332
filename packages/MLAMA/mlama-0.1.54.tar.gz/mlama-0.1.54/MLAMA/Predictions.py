# -*- coding: utf-8 -*-
"""Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_OWOjyFLje8iOhoCfpmJmMRDhOX6tLu
"""

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, arima_res_xgb, check_stationarity
from .Processor import process_metrics_df, create_prediction_df, replace_negatives

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import pandas as pd
import numpy as np

#Currently Prediction.py, I might move this to Models

#### Prediction on Wave 1,2,3 Data [Prediction 1,2,3,4,5,6 weeks]

#tuned_model_dict contains param for specific delay, wave all models
#generalized version
def wave_prediction(delay_definition_flag, delayed_start_matrix, predictions, prediction_length, delay, WAVE, waveID,
                    training_period, lag_reserve, lagged_amount, overlap, confidence_interval, tuned_model_dict, models):
    """
    Predicts weekly cases for multiple models dynamically.

    Args:
        delay_definition_flag: Flag for delay definition. Delay/Responsiveness.
        predictions: List of prediction steps.
        prediction_length: Length of the prediction.
        delay: Delay in weeks.
        WAVE: Wave data object.
        waveID: Identifier for the wave.
        training_period: Training period length.
        lag_reserve: Reserve for lag data.
        lagged_amount: How much past data is used.
        tuned_model_dict: Dictionary of tuned parameters for all models.
        models: Dictionary of models with their initialization functions. Example: {'ARIMA': SARIMAX, 'RF': RandomForestRegressor}.

    Returns:
        metric_row: Dictionary containing metrics and predictions for all models.
    """
    print("Prediction Length:", prediction_length, "Delay:", delay, "Wave:", waveID)

    # Generate wave, train, and test data with delay
    wave, train, test = WAVE.get_wave_dates_with_delay(delay, delayed_start_matrix, delay_definition_flag)
    single_train, full_train, fit_weeks_stat = stats_data(train) #single_ means only one feature weekcase is used
    single_test, full_test, pred_weeks_stat = stats_data(test.head(prediction_length))
    #print('**fit_pred_weeks_stat')
    fit_pred_weeks_stat = pd.concat([fit_weeks_stat.tail(training_period), pred_weeks_stat], axis = 0).reset_index(drop = True)
    #print('sungle_test ', single_test.shape)
    #print(fit_pred_weeks_stat.shape)
    fit_pred_y_STAT = pd.concat([single_train[-training_period:], single_test], axis = 0).reset_index(drop = True)
    print('fit_pred_y_STAT', fit_pred_y_STAT)
    # Output metrics container
    metric_row = {}

    #### ARIMA Model
    if 'ARIMA' in models:
        arima_params = tuned_model_dict['ARIMA']
        print("ARIMA Parameters:", arima_params)

        ARMAmodel = SARIMAX(single_train,
                            order=arima_params['order'],
                            seasonal_order=arima_params['seasonal_order'])
        arima_mse, arima_rmse, arima_mae, arima_mape, arima_list, arima, arimap, arimaf = ARIMA_model(ARMAmodel, single_test,
                                                                                       training_period + prediction_length,
                                                                                       confidence_interval)

        arimap = arimap.reset_index(drop=True)
        arima = arima.reset_index(drop=True)

        #arima.columns = ['ARIMA']

        #arima_error = pd.DataFrame(arima_list[0])
        print(arima_list)
        metric_row['ARIMA'] = {
            'MSE': arima_mse,
            'RMSE': arima_rmse,
            'MAE': arima_mae,
            'MAPE': arima_mape,
            'Predictions': arimap,
            'Fit': arimaf,
            'List': arima_list,
            'Weeks': fit_pred_weeks_stat,
            'Observed': fit_pred_y_STAT,
            'Fit_Pred': arima
        }

    #### Generate ML Data
    shift = 0
    if delay_definition_flag=='Delay':
        shift = delay
    train_x, test_x, train_y, test_y, fit_weeks_ml, pred_weeks_ml = gen_ml(wave, delay_definition_flag, prediction_length, lagged_amount,
                                                                           prediction_length, predictions, False, shift)
    fit_pred_weeks_ml = pd.concat([fit_weeks_ml.tail(training_period), pred_weeks_ml], axis=0).reset_index(drop=True)
    fit_pred_y_ML = pd.concat([train_y[-training_period:], test_y], axis=0).reset_index(drop=True)
    print('ARIMA, ', arima_list)
    #### Machine Learning Models
    for model_name, model_class in models.items():
        if model_name == 'ARIMA':
            continue  # ARIMA is handled separately

        # Extract parameters for the current model
        tuned_params = tuned_model_dict.get(model_name, {})
        print(f"Training {model_name} with Parameters:", tuned_params)

        # Initialize and fit the model dynamically
        model = model_class(**tuned_params)
        model.fit(train_x, train_y)

        # Predictions and metrics
        predictions = model.predict(test_x)

        mse = mean_squared_error(test_y, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(test_y, predictions)
        list_mape = np.abs((test_y - predictions) / test_y)
        mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100
        #mape = mean_absolute_percentage_error(data['weekcase'][-len(predict):].values,predict[:])
        #xgb_mape = mean_absolute_percentage_error(wave.weekcase[-len(xgb_predict):].values,xgb_predict[:])
        fit = model.predict(train_x)
        #fit = fit.reset_index(drop = True)
        fit = pd.DataFrame(fit).reset_index(drop = True).tail(training_period)
        fit = fit.reset_index(drop=True)

        predictions = pd.DataFrame(predictions).reset_index(drop = True)

        fit_pred = pd.concat([fit,predictions],axis=0)

        fit_pred = fit_pred.reset_index(drop=True)
        #print(fit_pred.head())

        #_list,_mse,_rmse,_mae,_mape, _predict = ml_model(test_x,test_y,model,train)

        print(model_name, 'ML, ', list_mape)

        # Store metrics
        metric_row[model_name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'Predictions': predictions,
            'Fit': fit,
            'List': list_mape,
            'Weeks': fit_pred_weeks_ml,
            'Observed': fit_pred_y_ML,
            'Fit_Pred': fit_pred
        }
    #print(metric_row['ARIMA'])
    return metric_row

# #generalized
# #should we return the models from the wave_prediction

# #waveID = 1
# #predictions.py
# def seperate_model_eval(WAVES, models, predictions, delay_list, tuned_model_names, tuned_model_dict, delay_definition_flag, delayed_start_matrix, training_period, lag_reserve, lagged_amount, overlap, confidence_interval):
#   '''
#   COLUMN_NAMES not needed, later baad dibo

#   '''
#   #metrics_df = pd.DataFrame(columns=COLUMN_NAMES)#initialize in the cell, so that any previous values if any are removed

#   model_evaluation_dictionary = {}

#   for WAVE in WAVES:
#     waveID = WAVE.waveID

#     for prediction_length in predictions:
#       wave_delay_tuned_params = {}
#       #print(prediction_length)
#       for delay in delay_list:#all delay done here
#         for tuned_model_name in tuned_model_names:
#           wave_delay_model_key = 'wave '+str(waveID)+' delay '+str(delay)+tuned_model_name
#           wave_delay_tuned_params[tuned_model_name]=tuned_model_dict[wave_delay_model_key]
#         #print(waveID, prediction_length, delay)

#         #wave, train, test = WAVE.get_wave_dates_with_delay(delay)#this should be done inside the function?
#         metric_row =  wave_prediction(
#             delay_definition_flag, delayed_start_matrix, predictions, prediction_length, delay, WAVE, waveID, training_period, lag_reserve, lagged_amount, overlap, confidence_interval, wave_delay_tuned_params, models)
#         #print(metric_row)

#         wave_delay_prediction_length_key = 'wave '+str(waveID)+' delay '+str(delay)+' prediction_length '+str(prediction_length)
#         model_evaluation_dictionary[wave_delay_prediction_length_key] = metric_row
#         #metrics_df = pd.concat([metrics_df, metric_row])#, ignore_index=True


#   base_columns = ['Wave', 'Delay', 'Length', 'Observed']

#   # Dynamically create visualization_columns
#   visualization_columns = base_columns + list(models.keys())

#   #visualization_columns = ['Wave', 'Delay', 'Length', 'Observed', 'XGBoost', 'RF', 'ARIMA']# xgb_resid = arima_boost, sequential arima and xgb
#   task_name = 'all'
#   concise_data, concise_data_dictionary = create_prediction_df(WAVES, predictions, delay_list, model_evaluation_dictionary, inFoldername_pre, task_name, filename)

#   return model_evaluation_dictionary, concise_data

#generalized
#should we return the models from the wave_prediction

#waveID = 1
#predictions.py

def seperate_model_eval(WAVES, models, predictions, delay_list, inFoldername_pre, filename, tuned_model_names, tuned_model_dict, delay_definition_flag, delayed_start_matrix, training_period, lag_reserve, lagged_amount, overlap, confidence_interval):
  '''
  COLUMN_NAMES not needed, later baad dibo

  '''
  #metrics_df = pd.DataFrame(columns=COLUMN_NAMES)#initialize in the cell, so that any previous values if any are removed

  model_evaluation_dictionary = {}

  for WAVE in WAVES:
    waveID = WAVE.waveID

    for prediction_length in predictions:
      wave_delay_tuned_params = {}
      #print(prediction_length)
      for delay in delay_list:#all delay done here
        for tuned_model_name in tuned_model_names:
          wave_delay_model_key = 'wave '+str(waveID)+' delay '+str(delay)+tuned_model_name
          wave_delay_tuned_params[tuned_model_name]=tuned_model_dict[wave_delay_model_key]
        #print(waveID, prediction_length, delay)

        #wave, train, test = WAVE.get_wave_dates_with_delay(delay)#this should be done inside the function?
        metric_row =  wave_prediction(
            delay_definition_flag, delayed_start_matrix, predictions, prediction_length, delay, WAVE, waveID, training_period, lag_reserve, lagged_amount, overlap, confidence_interval, wave_delay_tuned_params, models)
        #print(metric_row)

        wave_delay_prediction_length_key = 'wave '+str(waveID)+' delay '+str(delay)+' prediction_length '+str(prediction_length)
        model_evaluation_dictionary[wave_delay_prediction_length_key] = metric_row
        #metrics_df = pd.concat([metrics_df, metric_row])#, ignore_index=True


  base_columns = ['Wave', 'Delay', 'Length', 'Observed']

  # Dynamically create visualization_columns
  visualization_columns = base_columns + list(models.keys())

  #visualization_columns = ['Wave', 'Delay', 'Length', 'Observed', 'XGBoost', 'RF', 'ARIMA']# xgb_resid = arima_boost, sequential arima and xgb
  task_name = 'all'
  concise_data, concise_data_dictionary = create_prediction_df(WAVES, predictions, delay_list, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename)

  return model_evaluation_dictionary, concise_data

#generalized version
#predictions.py


from collections import defaultdict
import pandas as pd

def individual_model_weekly_count(WAVES, predictions, delay_list, model_evaluation_dictionary, models, recent_week_count):
    """
    Generalized function to process weekly counts for multiple models, storing results by model, wave, and delay.

    Args:
        WAVES (list): List of WAVE objects.
        predictions (list): List of prediction lengths.
        delay_list (list): List of delays.
        model_evaluation_dictionary (dict): Dictionary containing model evaluation results.
        models (dict): Dictionary of model names and their corresponding classes.

    Returns:
        model_results (dict): A dictionary where keys are model names, and values are dictionaries
                              containing processed results for each WAVE and delay.
                              model_results[model_name][waveID][delay] = processed DataFrame
    """
    # Initialize dictionary to store results for each model, wave, and delay
    model_results = {model_name: defaultdict(dict) for model_name in models.keys()}

    for WAVE in WAVES:
        waveID = WAVE.waveID  # Extract wave ID

        for delay in delay_list:
            # Initialize storage for predictions of each model
            model_frames = {model_name: [] for model_name in models.keys()}
            # Initialize a dictionary to store the most recent fit_pred_weeks for each model
            most_recent_fit_pred_weeks = {model_name: None for model_name in models.keys()}

            for prediction_length in predictions:
                # Generate key for accessing the metrics
                wave_delay_prediction_key = f'wave {waveID} delay {delay} prediction_length {prediction_length}'

                # Check if key exists in the dictionary
                if wave_delay_prediction_key not in model_evaluation_dictionary:
                    print(f"Warning: Key {wave_delay_prediction_key} not found in model_evaluation_dictionary.")
                    continue

                metrics_row = model_evaluation_dictionary[wave_delay_prediction_key]

                # Extract results for each model dynamically
                for model_name in models.keys():
                    if model_name not in metrics_row:
                        print(f"Warning: Model {model_name} not found in metrics for {wave_delay_prediction_key}.")
                        continue

                    model_prediction = metrics_row[model_name]['Fit_Pred']
                    fit_pred_weeks = metrics_row[model_name]['Weeks']

                    if isinstance(model_prediction, pd.DataFrame):  # Ensure it's a DataFrame
                        model_prediction.columns = [f'{model_name}_{prediction_length}']
                        model_frames[model_name].append(model_prediction)

                        # Store the most recent fit_pred_weeks for each model
                        most_recent_fit_pred_weeks[model_name] = fit_pred_weeks
                    else:
                        print(f"Warning: Model {model_name} prediction is not a DataFrame for {wave_delay_prediction_key}.")

            # Append the most recent fit_pred_weeks for each model after the loop
            for model_name in models.keys():
                if most_recent_fit_pred_weeks[model_name] is not None:
                    model_frames[model_name].append(most_recent_fit_pred_weeks[model_name])

            # Process predictions for each model
            for model_name, frames in model_frames.items():
                if frames:  # Only process if there are valid frames
                    processed_df = process_metrics_df(WAVE, frames, recent_week_count)
                    #processed_df = processed_df.applymap(replace_negatives)
                    model_results[model_name][waveID][delay] = processed_df  # Store with delay key

    return model_results

# def individual_model_weekly_count(WAVES, predictions, delay_list, model_evaluation_dictionary, models, recent_week_count):
#     """
#     Generalized function to process weekly counts for multiple models.

#     Args:
#         WAVES (list): List of WAVE objects.
#         predictions (list): List of prediction lengths.
#         delay_list (list): List of delays.
#         model_evaluation_dictionary (dict): Dictionary containing model evaluation results.
#         models (dict): Dictionary of model names and their corresponding classes.
#         recent_week_count (int): Number of recent weeks to consider for calculation.

#     Returns:
#         model_results (dict): A dictionary where keys are model names, and values are dictionaries
#                               containing processed results for each WAVE.
#     """
#     # Initialize dictionary to store results for each model, wave, and delay
#     model_results = {model_name: defaultdict(dict) for model_name in models.keys()}

#     for WAVE in WAVES:
#         waveID = WAVE.waveID  # Extract wave ID

#         for delay in delay_list:
#             # Initialize storage for predictions of each model
#             model_frames = {model_name: [] for model_name in models.keys()}

#             for prediction_length in predictions:
#                 # Generate key for accessing the metrics
#                 wave_delay_prediction_key = f'wave {waveID} delay {delay} prediction_length {prediction_length}'

#                 # Check if key exists in the dictionary
#                 if wave_delay_prediction_key not in model_evaluation_dictionary:
#                     print(f"Warning: Key {wave_delay_prediction_key} not found in model_evaluation_dictionary.")
#                     continue

#                 metrics_row = model_evaluation_dictionary[wave_delay_prediction_key]

#                 # Extract results for each model dynamically
#                 for model_name in models.keys():
#                     if model_name not in metrics_row:
#                         print(f"Warning: Model {model_name} not found in metrics for {wave_delay_prediction_key}.")
#                         continue

#                     model_prediction = metrics_row[model_name]['Fit_Pred']

#                     if isinstance(model_prediction, pd.DataFrame):  # Ensure it's a DataFrame
#                         model_prediction.columns = [f'{model_name}_{prediction_length}']
#                         model_frames[model_name].append(model_prediction)
#                     else:
#                         print(f"Warning: Model {model_name} prediction is not a DataFrame for {wave_delay_prediction_key}.")

#             # Process predictions for each model
#             for model_name, frames in model_frames.items():
#                 if frames:  # Only process if there are valid frames
#                     processed_df = process_metrics_df(WAVE, frames, recent_week_count)
#                     processed_df = processed_df.applymap(replace_negatives)
#                     model_results[model_name][waveID][delay] = processed_df  # Store with delay key

#     return model_results
    # # Initialize dictionaries to store results for each model
    # model_results = {model_name: {} for model_name in models.keys()}

    # for WAVE in WAVES:
    #     waveID = WAVE.waveID

    #     for delay in delay_list:
    #         # Initialize containers for predictions of each model
    #         model_frames = {model_name: [] for model_name in models.keys()}

    #         for prediction_length in predictions:
    #             # Generate key for accessing the metrics
    #             wave_delay_key = f'wave {waveID} delay {delay} prediction_length {prediction_length}'
    #             metrics_row = model_evaluation_dictionary[wave_delay_key]

    #             # Extract results for each model dynamically
    #             for model_name in models.keys():
    #                 #print("Wave: ", waveID, "Delay: ", delay, 'model_name: ', model_name)
    #                 #print(metrics_row[model_name])
    #                 model_prediction = metrics_row[model_name]['Fit_Pred']#.values[0]
    #                 model_prediction.columns = [f'{model_name}_{prediction_length}']#this didn't work for ARIMA
    #                 print(model_prediction.columns)
    #                 model_frames[model_name].append(model_prediction)

    #         # Process predictions for each model
    #         for model_name, frames in model_frames.items():
    #             processed_df = process_metrics_df(WAVE, frames, recent_week_count)
    #             processed_df = processed_df.applymap(replace_negatives)
    #             model_results[model_name][waveID] = processed_df

    # return model_results

#generalized
def individual_model_weekly_MAPE(WAVES, predictions, delay_list, model_evaluation_dictionary, models):
    """
    Generalized function for calculating weekly MAPE values for multiple models.

    Args:
        WAVES: List of wave objects, each with a unique waveID.
        predictions: List of prediction lengths.
        delay_list: List of delays.
        model_evaluation_dictionary: Dictionary containing evaluation metrics for models.
        models: Dictionary mapping model names to their respective prediction keys in model_evaluation_dictionary.

    Returns:
        A dictionary with model names as keys and corresponding weekly MAPE dataframes as values.
    """
    # Initialize a dictionary to hold results for all models
    model_results_wave_collection = {model: {} for model in models}

    weeks_column = [x for x in predictions]

    #print(weeks_column)
    #print(predictions)

    for WAVE in WAVES:  # Process each wave
        waveID = WAVE.waveID

        # Initialize per-wave collections for each model
        model_results_collection = {model: {} for model in models}

        for delay in delay_list:  # Process each delay
            print('DELAY:', delay)

            # Initialize per-delay lists for each model
            all_week_model_lists = {model: [] for model in models}

            for week in predictions:  # Process each week
                print('WEEK:', week)

                # Initialize per-week lists for each model
                week_model_lists = {model: [] for model in models}

                for prediction_length in predictions:  # Process each prediction length
                    print('PREDICTION LENGTH:', prediction_length)

                    # Initialize weekly values for each model
                    weekly_model_delay_values = {model: np.nan for model in models}

                    if prediction_length >= week:
                        # Construct the key to fetch metrics
                        wave_delay_prediction_length_key = f'wave {WAVE.waveID} delay {delay} prediction_length {prediction_length}'
                        metrics_row = model_evaluation_dictionary[wave_delay_prediction_length_key]
                        #print(metrics_row)
                        for model_name in models.keys():
                            model_list = metrics_row[model_name]['List']
                            print(model_name, model_list)
                            # Calculate weekly value
                            weekly_model_delay_values[model_name] = (
                                np.array(model_list)[week-1] / prediction_length * 100
                            )

                    # Append weekly values to the week lists for each model
                    for model in models:
                        week_model_lists[model].append(weekly_model_delay_values[model])

                # Append week lists to the all-week lists for each model
                for model in models:
                    all_week_model_lists[model].append(week_model_lists[model])

            # Create dataframes for all-week lists and store them in the delay collections
            for model in models:
                model_results_collection[model][delay] = pd.DataFrame(
                    [all_week_model_lists[model]], columns=weeks_column
                )

        # Store delay collections in the wave collections
        for model in models:

            model_results_wave_collection[model][waveID] = model_results_collection[model]

    return model_results_wave_collection

