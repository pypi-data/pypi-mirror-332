import torch
from torch import nn

from .base_path import BasePath
from .linear import LinearPath

class Attentionpath(BasePath):
    """
    Attention path class.

    Args:
        n_embed (int, optional): Number of embedding dimensions. Defaults to 32.
        depth (int, optional): Depth of the MLP. Defaults to 3.
        base: Path class to correct. Defaults to LinearPath.
    """
    def __init__(
        self,
        n_embed: int = 32,
        depth: int = 3,
        base: BasePath = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        
        self.n_embed = n_embed
        self.depth = depth

        self.layers = []
        self.layers.append(nn.Linear(1, n_embed, dtype=torch.float64, bias=True))
        for i in range(depth):
            self.layers.append(nn.MultiheadAttention(n_embed, 1, batch_first=True))
            self.layers.append(ResNetLayer(n_embed))
        self.layers.append(nn.Linear(n_embed, 1, dtype=torch.float64, bias=True))

        self.layers = nn.Sequential(*self.layers)
        self.layers.to(self.device)

        self.neval = 0

        self.base = base if base is not None else LinearPath(**kwargs)

        print("Number of trainable parameters in MLP:", sum(p.numel() for p in self.parameters() if p.requires_grad))
        print(self.layers)

    def get_geometry(self, time: float, *args):
        """
        Generates a geometric path using the MLP.

        Args:
            time (float): Time parameter for generating the path.
            *args: Additional arguments.

        Returns:
            torch.Tensor: The geometric path generated by the MLP.
        """
        out = self.layers(time.unsqueeze(-1)).squeeze(-1) * time * (1 - time)
        
        out = self.base.get_geometry(time) + out
        return out

class ResNetLayer(nn.Module):
    def __init__(
        self,
        output_size: int,
    ):
        super().__init__()
        self.layer = SwiGLU(output_size, output_size)

    def forward(self, x):
        return x + self.layer(x)
    
class SwiGLU(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.fc1 = nn.Linear(in_features, out_features)
        self.fc2 = nn.Linear(in_features, out_features)
        self.activation = nn.SiLU()

    def forward(self, x):
        return self.fc1(x) * self.activation(self.fc2(x))