{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Code speed and efficiency\nEMD analysis can be time-consuming. This tutorial outlines some basic\ninformation about how long different computations may take and what features\ncan be used to speed this up.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sift Speed\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sift can be time-consuming for two reasons. Firstly, it is an iterative\nprocess which can vary in how long it takes to converge. Though many signals\ncan be sifted in a handful of iterations some may take tens or hundreds of\niterations before an IMF is identified - unfortunately we can't tell before\nthe process is running. Secondly, the sift is sequential in that we can't\ncompute the second IMF until the first IMF has been identified. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default settings in the sift are selected to operate reasonably well and\nreasonable quickly on a signal. Here we include some a very rough, order of\nmagnitude illustration of timings based on running speeds on a modern\ncomputer (the readthedocs server generating this website).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import emd\nimport time\nimport numpy as np\n\n# ---- Five thousand samples example\nx = np.random.randn(5000,)\n\nt = time.process_time()\nimf = emd.sift.sift(x)\nelapsed = time.process_time() - t\nprint('{0} samples sifted in {1} seconds'.format(5000, elapsed))\n\n# ---- fifty thousand samples example\nx = np.random.randn(50000,)\n\nt = time.process_time()\nimf = emd.sift.sift(x)\nelapsed = time.process_time() - t\nprint('{0} samples sifted in {1} seconds'.format(50000, elapsed))\n\n# ---- one hundred thousand sample example\nx = np.random.randn(1000000,)\n\nt = time.process_time()\nimf = emd.sift.sift(x)\nelapsed = time.process_time() - t\nprint('{0} samples sifted in {1} seconds'.format(100000, elapsed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sift executes in well less than a second for all examples. Computation\ntime increases with input array size linearly for relatively short input but\nexponentially but larger ones (>1 million samples).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some options can noticeably slow down the sift. For example, the imf option\n``imf_opts/stop_method='rilling' is tends to use more iterations than the\ndefault ``imf_opts/stop_method='sd'``. Similarly changing the thresholds for\neither stopping method can increase the number of iterations computed. the\nenvelope interpolation method ``envelope_opes/interp_method='mono_pchip'`` is\nmuch slower than the default ``envelope_opes/interp_method='splrep'``\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sift Variants\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to the classic sift, the ensemble and mask sift are slower but have\nmore options for speeding up computation. The computation speed of\n``emd.sift.ensemble_sift`` and ``emd.sift.complete_ensemble_sift`` is most\nstrongly determined by the number of ensembles that are computed - however,\nthese can be parallelised by setting the ``nprocesses`` option to be greater\nthan 1.\n\nSimilarly, the timing of ``emd.sift.mask_sift`` is strongly determined by the\nnumber of separate masks applied to each IMF - specified by ``nphases``.\nAgain this can be parallelised by setting ``nprocesses`` to speed up\ncomputation time.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparse Time-Frequency Transforms.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another potentially slow computation during an EMD analysis is generating\nHilbert-Huang and Holospectrum arrays. Both of these algorithms make use of\nnested looping to form the output. As this can be very slow, these operations\nare accelerated internally by using sparse arrays. This allows the\nHilbert-Huang transform and Holospectrum arrays to be formed in one shot\nwithout looping.\n\nBy default, these outputs are cast to normal numpy arrays before being\nreturned to the user. If you are working with a very large transform, it is\nfar more memory and computationally efficient to work with the sparse form of\nthese arrays. These can be returned by specifying ``return_sparse=True`` in\nthe options in either ``emd.spectra.hilberthuang`` or\n``emd.spectra.holospectrum``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}