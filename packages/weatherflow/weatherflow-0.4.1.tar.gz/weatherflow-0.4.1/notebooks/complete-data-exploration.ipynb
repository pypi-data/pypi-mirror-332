{
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# ERA5 Data Exploration for Weather Flow Matching\n",
       "\n",
       "This notebook demonstrates how to load, explore, and visualize ERA5 data for weather prediction using the WeatherFlow library. We'll cover:\n",
       "\n",
       "1. Loading data from WeatherBench2\n",
       "2. Exploring the data structure\n",
       "3. Visualizing different variables\n",
       "4. Preparing data for model training\n",
       "5. Computing statistics and climatology\n",
       "\n",
       "Let's get started!"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Installation\n",
       "\n",
       "First, let's make sure we have WeatherFlow and all dependencies installed."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Install WeatherFlow if needed\n",
       "try:\n",
       "    import weatherflow\n",
       "    print(f\"WeatherFlow version: {weatherflow.__version__}\")\n",
       "except ImportError:\n",
       "    !pip install -e ..\n",
       "    import weatherflow\n",
       "    print(f\"WeatherFlow installed, version: {weatherflow.__version__}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import required libraries\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import torch\n",
       "import xarray as xr\n",
       "import cartopy.crs as ccrs\n",
       "from tqdm.notebook import tqdm\n",
       "import os\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')  # Suppress some warnings for cleaner output\n",
       "\n",
       "# Import WeatherFlow modules\n",
       "from weatherflow.data import ERA5Dataset, create_data_loaders\n",
       "from weatherflow.utils import WeatherVisualizer\n",
       "\n",
       "# Set up matplotlib larger figures\n",
       "plt.rcParams['figure.figsize'] = (14, 8)\n",
       "plt.rcParams['figure.dpi'] = 100"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Loading ERA5 Data\n",
       "\n",
       "WeatherFlow supports loading ERA5 data from multiple sources:\n",
       "\n",
       "1. WeatherBench2 on Google Cloud Storage\n",
       "2. Local NetCDF files\n",
       "3. Custom Zarr datasets\n",
       "\n",
       "Let's use the WeatherBench2 dataset which contains preprocessed global ERA5 reanalysis data."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define variables and pressure levels we're interested in\n",
       "variables = ['z', 't', 'u', 'v']  # Geopotential, temperature, u-wind, v-wind\n",
       "pressure_levels = [500]  # 500 hPa level\n",
       "years = ('2016', '2016')  # Load just one year for faster exploration\n",
       "\n",
       "# Detailed explanation of variables:\n",
       "variable_details = {\n",
       "    'z': 'Geopotential (m²/s²) - Represents atmospheric pressure levels',\n",
       "    't': 'Temperature (K) - Air temperature',\n",
       "    'u': 'U-component of wind (m/s) - Eastward wind',\n",
       "    'v': 'V-component of wind (m/s) - Northward wind',\n",
       "    'q': 'Specific humidity (kg/kg) - Mass of water vapor per unit mass of air',\n",
       "    'r': 'Relative humidity (%) - Amount of water vapor relative to maximum possible'\n",
       "}\n",
       "\n",
       "# Print selected variables and their descriptions\n",
       "print(\"Selected variables:\")\n",
       "for var in variables:\n",
       "    print(f\"  - {var}: {variable_details.get(var, 'Unknown variable')}\")\n",
       "print(f\"\\nPressure level: {pressure_levels[0]} hPa\")\n",
       "print(f\"Time period: {years[0]} to {years[1]}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load ERA5 data with progress information\n",
       "print(\"Loading ERA5 data from WeatherBench2...\")\n",
       "try:\n",
       "    # Try loading with default settings\n",
       "    era5_data = ERA5Dataset(\n",
       "        variables=variables,\n",
       "        pressure_levels=pressure_levels,\n",
       "        time_slice=years,\n",
       "        normalize=False,  # Keep original values for exploration\n",
       "        verbose=True\n",
       "    )\n",
       "    print(f\"Successfully loaded data with {len(era5_data)} time steps\")\n",
       "except Exception as e:\n",
       "    print(f\"Error loading data: {str(e)}\")\n",
       "    print(\"\\nTrying alternative loading method...\")\n",
       "    \n",
       "    # If default method fails, try with explicit storage options\n",
       "    era5_data = ERA5Dataset(\n",
       "        variables=variables,\n",
       "        pressure_levels=pressure_levels,\n",
       "        time_slice=years,\n",
       "        normalize=False,\n",
       "        verbose=True,\n",
       "        add_physics_features=False\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Exploring Data Structure\n",
       "\n",
       "Let's examine the structure of the loaded data to better understand what we're working with."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get basic dataset information\n",
       "print(f\"Dataset shape information:\")\n",
       "print(f\"  - Number of time steps: {len(era5_data)}\")\n",
       "print(f\"  - Variables: {era5_data.variables}\")\n",
       "print(f\"  - Pressure levels: {era5_data.pressure_levels}\")\n",
       "print(f\"  - Spatial grid size: {era5_data.ds.latitude.size} × {era5_data.ds.longitude.size}\")\n",
       "\n",
       "# Look at the first sample to understand its structure\n",
       "sample = era5_data[0]\n",
       "print(\"\\nSample data structure:\")\n",
       "for key, value in sample.items():\n",
       "    if isinstance(value, dict):\n",
       "        print(f\"  - {key}: {type(value)}\")\n",
       "        for subkey, subvalue in value.items():\n",
       "            print(f\"      {subkey}: {type(subvalue)}\")\n",
       "    else:\n",
       "        print(f\"  - {key}: {type(value)}, shape: {value.shape}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract coordinate information\n",
       "coords = era5_data.get_coords()\n",
       "lats = coords['latitude']\n",
       "lons = coords['longitude']\n",
       "\n",
       "print(f\"Latitude range: {lats.min():.2f}° to {lats.max():.2f}°, {len(lats)} points\")\n",
       "print(f\"Longitude range: {lons.min():.2f}° to {lons.max():.2f}°, {len(lons)} points\")\n",
       "\n",
       "# Show coordinate spacing (important for certain physical calculations)\n",
       "lat_spacing = np.mean(np.diff(lats))\n",
       "lon_spacing = np.mean(np.diff(lons))\n",
       "print(f\"Grid resolution: {lat_spacing:.2f}° latitude × {lon_spacing:.2f}° longitude\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Visualizing Weather Variables\n",
       "\n",
       "Now let's visualize each of our variables to get a feel for the data. We'll use the WeatherVisualizer class from WeatherFlow for this."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize visualizer\n",
       "visualizer = WeatherVisualizer(figsize=(14, 8))\n",
       "\n",
       "# Extract first sample (current state)\n",
       "sample_data = era5_data[0]['input']\n",
       "\n",
       "# Create a dictionary for visualization\n",
       "data_dict = {}\n",
       "for i, var in enumerate(variables):\n",
       "    # Each variable has shape [levels, lat, lon], select first level\n",
       "    data_dict[var] = sample_data[i, 0].numpy()  # Convert tensor to numpy\n",
       "\n",
       "# Plot each variable\n",
       "for i, var_name in enumerate(variables):\n",
       "    plt.figure(figsize=(14, 8))\n",
       "    fig, ax = visualizer.plot_field(\n",
       "        data_dict[var_name],\n",
       "        title=f\"{var_name} at {pressure_levels[0]} hPa\",\n",
       "        var_name=var_name,\n",
       "        coastlines=True,\n",
       "        grid=True\n",
       "    )\n",
       "    plt.tight_layout()\n",
       "    plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 4.1 Wind Vector Visualization\n",
       "\n",
       "Since we have both U and V wind components, we can visualize the vector field to see wind patterns."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract U and V wind components\n",
       "u_index = variables.index('u')\n",
       "v_index = variables.index('v')\n",
       "u_wind = sample_data[u_index, 0].numpy()\n",
       "v_wind = sample_data[v_index, 0].numpy()\n",
       "\n",
       "# For background, use geopotential height\n",
       "z_index = variables.index('z')\n",
       "geopotential = sample_data[z_index, 0].numpy()\n",
       "\n",
       "# Calculate wind speed (magnitude)\n",
       "wind_speed = np.sqrt(u_wind**2 + v_wind**2)\n",
       "\n",
       "# Plot wind field with geopotential height as background\n",
       "fig, ax = visualizer.plot_flow_vectors(\n",
       "    u_wind, v_wind, \n",
       "    background=geopotential, \n",
       "    var_name='z',\n",
       "    title=f\"Wind Field at {pressure_levels[0]} hPa\",\n",
       "    scale=1.0, \n",
       "    density=1.0\n",
       ")\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Plot wind speed\n",
       "plt.figure(figsize=(14, 8))\n",
       "fig, ax = visualizer.plot_field(\n",
       "    wind_speed,\n",
       "    title=f\"Wind Speed at {pressure_levels[0]} hPa\",\n",
       "    cmap='YlOrRd',\n",
       "    coastlines=True,\n",
       "    grid=True\n",
       ")\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Temporal Evolution\n",
       "\n",
       "Let's look at how variables change over time by extracting and visualizing a sequence of states."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Number of time steps to visualize\n",
       "n_steps = 5\n",
       "\n",
       "# Extract a sequence of states for one variable (geopotential)\n",
       "var_index = 0  # Index of variable to visualize (geopotential)\n",
       "level_index = 0  # First pressure level\n",
       "\n",
       "# Collect time sequence\n",
       "time_sequence = []\n",
       "time_stamps = []\n",
       "\n",
       "for i in range(n_steps):\n",
       "    if i < len(era5_data):\n",
       "        sample = era5_data[i]\n",
       "        # Extract the variable\n",
       "        time_sequence.append(sample['input'][var_index, level_index].numpy())\n",
       "        # Extract timestamp from metadata\n",
       "        time_stamps.append(sample['metadata']['t0'])\n",
       "\n",
       "# Create animation\n",
       "print(f\"Creating animation for {variables[var_index]} at {pressure_levels[0]} hPa...\")\n",
       "anim = visualizer.create_prediction_animation(\n",
       "    time_sequence,\n",
       "    var_name=variables[var_index],\n",
       "    title=f\"{variables[var_index]} Evolution\",\n",
       "    interval=800  # Slower animation for better viewing\n",
       ")\n",
       "\n",
       "# Display animation\n",
       "from IPython.display import HTML\n",
       "HTML(anim.to_jshtml())"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Data Statistics and Climatology\n",
       "\n",
       "Understanding the statistical properties of each variable is important for normalization and model training."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate statistics for each variable\n",
       "stats = {}\n",
       "\n",
       "# Number of samples to use for statistics (limit for memory efficiency)\n",
       "n_samples = min(50, len(era5_data))\n",
       "print(f\"Computing statistics from {n_samples} samples...\")\n",
       "\n",
       "# Initialize arrays to collect data\n",
       "var_data = {var: [] for var in variables}\n",
       "\n",
       "# Collect data\n",
       "for i in tqdm(range(n_samples)):\n",
       "    sample = era5_data[i]\n",
       "    for j, var in enumerate(variables):\n",
       "        var_data[var].append(sample['input'][j].numpy().flatten())\n",
       "\n",
       "# Compute statistics\n",
       "for var in variables:\n",
       "    # Concatenate all samples for this variable\n",
       "    all_data = np.concatenate(var_data[var])\n",
       "    \n",
       "    # Calculate statistics\n",
       "    stats[var] = {\n",
       "        'mean': np.mean(all_data),\n",
       "        'std': np.std(all_data),\n",
       "        'min': np.min(all_data),\n",
       "        'max': np.max(all_data),\n",
       "        '5th_percentile': np.percentile(all_data, 5),\n",
       "        '95th_percentile': np.percentile(all_data, 95)\n",
       "    }\n",
       "\n",
       "# Display statistics\n",
       "print(\"\\nVariable Statistics:\")\n",
       "for var in variables:\n",
       "    print(f\"\\n{var} ({variable_details.get(var, '')})\")\n",
       "    for stat_name, stat_value in stats[var].items():\n",
       "        print(f\"  - {stat_name}: {stat_value:.2f}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize distributions\n",
       "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
       "axes = axes.flatten()\n",
       "\n",
       "for i, var in enumerate(variables):\n",
       "    # Get data for histograms\n",
       "    all_data = np.concatenate(var_data[var])\n",
       "    \n",
       "    # Plot histogram\n",
       "    axes[i].hist(all_data, bins=50, alpha=0.7, density=True)\n",
       "    axes[i].set_title(f\"{var} Distribution\")\n",
       "    axes[i].set_xlabel(variable_details.get(var, var))\n",
       "    axes[i].set_ylabel(\"Density\")\n",
       "    \n",
       "    # Add vertical lines for mean and std range\n",
       "    mean = stats[var]['mean']\n",
       "    std = stats[var]['std']\n",
       "    axes[i].axvline(mean, color='r', linestyle='--', label=f\"Mean: {mean:.2f}\")\n",
       "    axes[i].axvline(mean + std, color='g', linestyle=':', label=f\"±1 Std: {std:.2f}\")\n",
       "    axes[i].axvline(mean - std, color='g', linestyle=':')\n",
       "    axes[i].legend()\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Data Normalization and Preparation for Training\n",
       "\n",
       "Based on the statistics we calculated, let's create properly normalized data for model training."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create normalized data loaders for training\n",
       "print(\"Creating data loaders with normalization...\")\n",
       "\n",
       "# Split data into training and validation\n",
       "train_years = ('2016', '2016-06')  # First half of 2016\n",
       "val_years = ('2016-07', '2016-12')  # Second half of 2016\n",
       "\n",
       "# Create data loaders\n",
       "train_loader, val_loader = create_data_loaders(\n",
       "    variables=variables,\n",
       "    pressure_levels=pressure_levels,\n",
       "    train_slice=train_years,\n",
       "    val_slice=val_years,\n",
       "    batch_size=16,\n",
       "    num_workers=4,\n",
       "    normalize=True  # Apply normalization\n",
       ")\n",
       "\n",
       "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
       "print(f\"Validation samples: {len(val_loader.dataset)}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize normalized data\n",
       "# Get a batch from the training loader\n",
       "sample_batch = next(iter(train_loader))\n",
       "\n",
       "# Plot normalized fields for each variable\n",
       "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
       "axes = axes.flatten()\n",
       "\n",
       "for i, var in enumerate(variables):\n",
       "    # Extract normalized field\n",
       "    normalized_field = sample_batch['input'][0, i, 0].numpy()\n",
       "    \n",
       "    # Plot\n",
       "    im = axes[i].imshow(normalized_field, cmap=visualizer.VAR_CMAPS.get(var, 'viridis'))\n",
       "    axes[i].set_title(f\"Normalized {var}\")\n",
       "    plt.colorbar(im, ax=axes[i])\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Temporal Patterns and Lag Correlation\n",
       "\n",
       "Understanding the temporal correlation in weather data is crucial for flow matching. Let's examine how variables evolve over short time periods."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Select a specific location (grid point) to examine\n",
       "lat_idx = len(lats) // 2  # Middle latitude (roughly equator)\n",
       "lon_idx = len(lons) // 2  # Middle longitude\n",
       "\n",
       "print(f\"Selected location: Latitude {lats[lat_idx]:.2f}°, Longitude {lons[lon_idx]:.2f}°\")\n",
       "\n",
       "# Extract time series for each variable at this location\n",
       "n_samples = min(100, len(era5_data))\n",
       "time_series = {var: [] for var in variables}\n",
       "timestamps = []\n",
       "\n",
       "for i in tqdm(range(n_samples)):\n",
       "    sample = era5_data[i]\n",
       "    timestamps.append(sample['metadata']['t0'])\n",
       "    \n",
       "    for j, var in enumerate(variables):\n",
       "        # Extract value at the selected location\n",
       "        value = sample['input'][j, 0, lat_idx, lon_idx].item()\n",
       "        time_series[var].append(value)\n",
       "\n",
       "# Convert timestamps to datetime objects for better plotting\n",
       "import pandas as pd\n",
       "datetimes = pd.to_datetime(timestamps)\n",
       "\n",
       "# Plot time series\n",
       "fig, axes = plt.subplots(len(variables), 1, figsize=(14, 12), sharex=True)\n",
       "\n",
       "for i, var in enumerate(variables):\n",
       "    axes[i].plot(datetimes, time_series[var], '-o', markersize=4)\n",
       "    axes[i].set_title(f\"{var} - {variable_details.get(var, '')}\")\n",
       "    axes[i].set_ylabel(var)\n",
       "    axes[i].grid(True)\n",
       "\n",
       "axes[-1].set_xlabel(\"Time\")\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate lag correlations to understand predictability\n",
       "max_lag = 10  # Maximum lag in time steps\n",
       "lag_corrs = {var: [] for var in variables}\n",
       "\n",
       "for var in variables:\n",
       "    # Get the time series data\n",
       "    ts = np.array(time_series[var])\n",
       "    \n",
       "    # Calculate autocorrelation for different lags\n",
       "    for lag in range(max_lag + 1):\n",
       "        if lag == 0:\n",
       "            # Correlation with itself is always 1\n",
       "            lag_corrs[var].append(1.0)\n",
       "        else:\n",
       "            # Compute correlation between original series and lagged series\n",
       "            corr = np.corrcoef(ts[lag:], ts[:-lag])[0, 1]\n",
       "            lag_corrs[var].append(corr)\n",
       "\n",
       "# Plot lag correlations\n",
       "plt.figure(figsize=(12, 6))\n",
       "lags = range(max_lag + 1)\n",
       "\n",
       "for var in variables:\n",
       "    plt.plot(lags, lag_corrs[var], 'o-', label=var)\n",
       "    \n",
       "plt.xlabel('Lag (time steps)')\n",
       "plt.ylabel('Autocorrelation')\n",
       "plt.title('Temporal Autocorrelation by Variable')\n",
       "plt.grid(True)\n",
       "plt.legend()\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Calculate cross-correlations between variables\n",
       "plt.figure(figsize=(14, 10))\n",
       "var_data = {}\n",
       "for var in variables:\n",
       "    var_data[var] = np.array(time_series[var])\n",
       "\n",
       "# Create a correlation matrix\n",
       "corr_matrix = np.zeros((len(variables), len(variables)))\n",
       "for i, var1 in enumerate(variables):\n",
       "    for j, var2 in enumerate(variables):\n",
       "        corr_matrix[i, j] = np.corrcoef(var_data[var1], var_data[var2])[0, 1]\n",
       "\n",
       "# Plot correlation matrix as a heatmap\n",
       "plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
       "plt.colorbar(label='Correlation Coefficient')\n",
       "plt.xticks(range(len(variables)), variables)\n",
       "plt.yticks(range(len(variables)), variables)\n",
       "plt.title('Cross-Correlation Between Variables')\n",
       "\n",
       "# Add correlation values as text\n",
       "for i in range(len(variables)):\n",
       "    for j in range(len(variables)):\n",
       "        plt.text(j, i, f'{corr_matrix[i, j]:.2f}', \n",
       "                 ha='center', va='center', \n",
       "                 color='white' if abs(corr_matrix[i, j]) > 0.5 else 'black')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Compute derived quantities for physics constraints"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(\"Computing some important derived physical quantities for flow matching...\")\n",
       "\n",
       "# Extract geopotential, temperature, and wind components\n",
       "z_index = variables.index('z')\n",
       "t_index = variables.index('t')\n",
       "u_index = variables.index('u')\n",
       "v_index = variables.index('v')\n",
       "\n",
       "sample = era5_data[0]\n",
       "z = sample['input'][z_index, 0].numpy()\nt = sample['input'][t_index, 0].numpy()\nu = sample['input'][u_index, 0].numpy()\nv = sample['input'][v_index, 0].numpy()\n\n# Calculate wind speed\nwind_speed = np.sqrt(u**2 + v**2)\n\n# Calculate vorticity (curl of wind field)\n# Simplified calculation using finite differences\ndy = 111000 * np.mean(np.diff(lats))  # Convert degrees to meters\ndx = 111000 * np.mean(np.diff(lons)) * np.cos(np.radians(np.mean(lats)))\n\n# Compute derivatives\ndudy = np.zeros_like(u)\ndvdx = np.zeros_like(v)\n\ndudy[1:-1, :] = (u[2:, :] - u[:-2, :]) / (2 * dy)\ndvdx[:, 1:-1] = (v[:, 2:] - v[:, :-2]) / (2 * dx)\n\n# Relative vorticity (curl of velocity)\nvorticity = dvdx - dudy\n\n# Calculate divergence (measure of mass continuity)\ndudx = np.zeros_like(u)\ndvdy = np.zeros_like(v)\n\ndudx[:, 1:-1] = (u[:, 2:] - u[:, :-2]) / (2 * dx)\ndvdy[1:-1, :] = (v[2:, :] - v[:-2, :]) / (2 * dy)\n\ndivergence = dudx + dvdy\n\n# Visualize these derived quantities\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# Plot original wind field\nvisualizer.plot_flow_vectors(u, v, background=z, var_name='z', \n                           title=\"Wind Field and Geopotential Height\", \n                           ax=axes[0, 0])\n\n# Plot vorticity\ncmap = 'RdBu_r'\nim = axes[0, 1].imshow(vorticity, cmap=cmap, origin='lower')\naxes[0, 1].set_title(\"Vorticity (1/s)\")\nplt.colorbar(im, ax=axes[0, 1])\n\n# Plot divergence\nim = axes[1, 0].imshow(divergence, cmap=cmap, origin='lower')\naxes[1, 0].set_title(\"Divergence (1/s)\")\nplt.colorbar(im, ax=axes[1, 0])\n\n# Plot wind speed\nim = axes[1, 1].imshow(wind_speed, cmap='viridis', origin='lower')\naxes[1, 1].set_title(\"Wind Speed (m/s)\")\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize data suitable for flow matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Sequential Data for Flow Matching\")\n",
    "\n",
    "# Extract consecutive states to visualize flow matching targets\n",
    "n_steps = 4  # Number of consecutive steps\n",
    "first_sample_idx = 0\n",
    "\n",
    "states = []\n",
    "for i in range(first_sample_idx, first_sample_idx + n_steps):\n",
    "    if i < len(era5_data):\n",
    "        sample = era5_data[i]\n",
    "        states.append(sample['input'][z_index, 0].numpy())  # Using geopotential\n",
    "\n",
    "# Compute the \"flow\" between consecutive states\n",
    "flows = []\n",
    "for i in range(len(states) - 1):\n",
    "    flow = states[i+1] - states[i]\n",
    "    flows.append(flow)\n",
    "\n",
    "# Visualize states and flows\n",
    "fig, axes = plt.subplots(2, n_steps-1, figsize=(18, 10))\n",
    "\n",
    "# Plot consecutive states\n",
    "for i in range(n_steps-1):\n",
    "    # Plot current state\n",
    "    im = axes[0, i].imshow(states[i], cmap='viridis', origin='lower')\n",
    "    axes[0, i].set_title(f\"State at t={i}\")\n",
    "    plt.colorbar(im, ax=axes[0, i])\n",
    "    \n",
    "    # Plot flow to next state\n",
    "    im = axes[1, i].imshow(flows[i], cmap='RdBu_r', origin='lower')\n",
    "    axes[1, i].set_title(f\"Flow t={i} → t={i+1}\")\n",
    "    plt.colorbar(im, ax=axes[1, i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save statistics for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean dictionary of normalization statistics\n",
    "norm_stats = {}\n",
    "for var in variables:\n",
    "    norm_stats[var] = {\n",
    "        'mean': stats[var]['mean'],\n",
    "        'std': stats[var]['std']\n",
    "    }\n",
    "    \n",
    "print(\"Normalization statistics for model training:\")\n",
    "for var, var_stats in norm_stats.items():\n",
    "    print(f\"  {var}: mean = {var_stats['mean']:.4f}, std = {var_stats['std']:.4f}\")\n",
    "\n",
    "# Save statistics to file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "with open('../data/normalization_stats.json', 'w') as f:\n",
    "    json.dump(norm_stats, f, indent=2)\n",
    "    \n",
    "print(f\"\\nStatistics saved to '../data/normalization_stats.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "In this notebook, we've explored ERA5 data for weather prediction using flow matching. We:\n",
    "\n",
    "1. Loaded and inspected ERA5 data from WeatherBench2\n",
    "2. Visualized different weather variables and their relationships\n",
    "3. Analyzed temporal patterns and correlations\n",
    "4. Computed physics-relevant derived quantities\n",
    "5. Prepared data for flow matching model training\n",
    "\n",
    "Next steps would be to train a flow matching model using this data, which we'll cover in the next notebook.\n",
    "\"\"\")"
   ]
  }
 ]
}