{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tasks\n",
    "\n",
    "Tasks are the basis of this library. A task is just a pair of a DatasetLoader and TrainableModel. Currently,\n",
    "we only support \"ClassificationTask\"s. \n",
    "You create one using a TaskBuilder, ensuring to add a model and dataset loader:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install rocelib",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from rocelib.datasets.ExampleDatasets import get_example_dataset\n",
    "from rocelib.tasks.TaskBuilder import TaskBuilder\n",
    "# Get your dataset\n",
    "dl = get_example_dataset(\"ionosphere\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using a simple neural net from RoCELib\n",
    "You can use a simple neural network which is created and trained within our library. We support Pytorch, Keras and Sklearn models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Pytorch Model\n",
    "ct = TaskBuilder().add_pytorch_model(34, [8], 1, dl).add_data(dl).build()\n",
    "\n",
    "#Keras Model\n",
    "# ct = TaskBuilder().add_keras_model(34, [8], 1, dl).add_data(dl).build()\n",
    "\n",
    "#Sklearn Model\n",
    "#For sklearn instead of passing in the model dimension, you pass the model type:\n",
    "#\"decision tree\", \"logistic regression\" or \"svm\":\n",
    "ct = TaskBuilder().add_sklearn_model(\"decision tree\", dl).add_data(dl).build()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The default task type is a classification task but you can adjust this using the TaskBuilder's ```set_task_type``` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing your own Model\n",
    "You can import your own pretrained Pytorch, Keras or Sklearn model to use in our library for recourse generation/evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing a Pytorch Model\n",
    "\n",
    "To import a .pt or .pth file:\n",
    "You must save your entire model, rather than just the state dictionary i.e. do:\n",
    "torch.save(model, \"./model.pt”)\n",
    "Rather than: torch.save(model.state_dict(), \"./model.pt”)\n",
    "To then load it into our library you can create a task using the ```add_model_from_path``` function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_model_from_path(\"model.pt\").add_data(dl).build()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing a Keras Model\n",
    "\n",
    "Use the ```add_model_from_path()``` function, providing a .keras file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_model_from_path(\"model.keras\").add_data(dl).build()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing a Sklearn Model\n",
    "\n",
    "Use the ```add_model_from_path()``` function, providing a .pkl file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_model_from_path(\"model.pkl\").add_data(dl).build()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Mutliple Models to the Task (for Model Multiplicity Evaluation)\n",
    "You can add multiple models to the task\n",
    "Note that the main model will be the first model that you added to the task (i.e. this is the model that counterfactuals will be generated for when you call ```ct.generate```)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = (TaskBuilder()\n",
    "      .add_pytorch_model(34, [8], 1, dl) #base model\n",
    "      .add_keras_model(34, [8], 1, dl)\n",
    "      .add_keras_model(34, [8], 1, dl).add_data(dl).build())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naming the imported Models\n",
    "You can optionally name your models when you import them. If you do not provide a name the library will generate a default name for the model e.g. \"pytorch_model_1\", \"sklearn_model_1\", \"keras_model_1\", \"pytorch_model_2\" etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = (TaskBuilder()\n",
    "      .add_pytorch_model(34, [8], 1, dl, \"my_model\")\n",
    "      .add_keras_model(34, [8], 1, dl)\n",
    "      .add_keras_model(34, [8], 1, dl).add_data(dl).build())\n",
    "\n",
    "print(ct.mm_models.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Counterfactuals for the Base Model\n",
    "To generate counterfactuals, you use the `ct.generate` method, \n",
    "You can pass in the list of recourse methods you would like to generate counterfactuals for as well as any kwargs. (use `ct.get_recourse_methods()` to see all supported recourse methods)\n",
    "If you do not provide a list of recourse methods, counterfactuals will be generated for all recourse methods\n",
    "This will store the counterfactuals within the task for use when evaluating later and will also return them as a dictionary from recourse method to a tuple of (Pandas dataframe holding the counterfactual, time taken to generate the counterfactual)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct.get_recourse_methods()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Generate counterfactuals for MCE and BinaryLinearSearch methods\n",
    "ces = ct.generate([\"MCE\", \"NNCE\"])\n",
    "print(ces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Generate counterfactuals for all recourse methods\n",
    "# ces = ct.generate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#An example with kwargs:\n",
    "res = ct.generate([\"BinaryLinearSearch\"], column_name=\"HiringDecision\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Counterfactuals for All Models\n",
    "To generate counterfactuals for all models, you must have added multiple models to the task, else an error will be printed when you attempt to generate counterfactuals. Then you can use the `ct.generate_mm` method.\n",
    "You can pass in the list of recourse methods you would like to generate counterfactuals for as well as any kwargs. (use `ct.get_recourse_methods()` to see all supported recourse methods)\n",
    "If you do not provide a list of recourse methods, counterfactuals will be generated for all recourse methods\n",
    "This will store the counterfactuals within the task for use when evaluating later and will also return them as a nested dictionary from recourse method to model name to a tuple of (Pandas dataframe holding the counterfactual, time taken to generate the counterfactual)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_pytorch_model(34, [8], 1, dl).add_keras_model(34, [8], 1, dl).add_data(dl).build() #Ensure your task has multiple models\n",
    "ces = ct.generate_mm([\"MCE\", \"NNCE\"])\n",
    "print(ces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Evaluation Benchmarking Metrics\n",
    "To generate evaluation metrics, you can use the `ct.evaluate(methods, evaluations)` method, which will print the evaluation metrics in a table.\n",
    "You can pass in the list of recourse methods you would like to evaluate and evaluation metrics you would like to use\n",
    "\n",
    "It is important to note that you can only evaluate recourse methods that you have already called `ct.generate` for. \n",
    "If you have not already generated counterfactuals for recourse method X, and then try to evaluate X, the library will skip recourse method X and print a message telling you to generate counterfactuals for it first. See the below example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_pytorch_model(34, [8], 1, dl).add_keras_model(34, [8], 1, dl).add_data(dl).build() #Ensure your task has multiple models\n",
    "ces = ct.generate([\"NNCE\"])\n",
    "#We have not generated counterfactuals for MCE, so when we try to evaluate MCE the library will skip it and only evaluate NNCE\n",
    "evals = ct.evaluate(methods=[\"MCE\", \"NNCE\"], evaluations=[\"Distance\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ces = ct.generate([\"MCE\"])\n",
    "# Sicne we have generated for MCE, we can now evaluate for MCE\n",
    "evals = ct.evaluate(methods=[\"MCE\", \"BinaryLinearSearch\"], evaluations=[\"Distance\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you do not provide the list of recourse methods, the library will evaluate all recourse methods that you have previously called `generate` on.\n",
    "If you do not provide the list of evaluation metrics, the library will use all applicable evaluation metrics\n",
    "To see all supported evaluation metrics, use `ct.get_evaluation_metrics` "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ces = ct.generate()\n",
    "# evals = ct.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Multiplicity Evaluation Metrics \n",
    "If you would like to evaluate model multiplicity robustness, you must have already called `ct.generate_mm` for all the recourse methods you wish to evaluate. Note: `ct.generate` will not suffice as that does not generate for all the stored models, you must call `ct.generate_mm`.  \n",
    "If you have not already called `generate_mm` for recourse method X, and then try to evaluate MM for X, the library will skip recourse method X and print a message telling you to call `generate_mm` for it first. See the below example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ct = TaskBuilder().add_pytorch_model(34, [8], 1, dl).add_pytorch_model(34, [8], 1, dl).add_data(dl).build()\n",
    "\n",
    "ces = ct.generate([\"MCE\"])\n",
    "ces = ct.generate_mm([\"NNCE\"])\n",
    "# Note we have not called generate_mm for MCE (generate does not count as that does not generate for all stored models)\n",
    "\n",
    "# We have not generated counterfactuals for MCE, so when we try to evaluate MCE the library will skip it and only evaluate NNCE (as we have called generate_MM for NNCE)\n",
    "evals = ct.evaluate(methods=[\"MCE\", \"NNCE\"], evaluations=[\"Distance\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualising Your Metrics\n",
    "To get a more visual representation of your evaluation metrics, run `ct.evaluate` with the `visualisation` flag set to `True`\n",
    "This will generate a bar chart and, if more than 3 evaluation metrics were used, a radar chart"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ces = ct.generate([\"MCE\", \"NNCE\", \"KDTreeNNCE\", \"Wachter\", \"RNCE\"])\n",
    "evals = ct.evaluate(visualisation=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Your Own Recourse Method\n",
    "To add your own recourse method, create the class for your recourse method. Your class should extend RecourseGenerator and implement the `_generation_method` method which generates a counterfactual for a single instance.\n",
    "You should then call `ct.add_recourse_method(method name, method class)`\n",
    "You can now use the library to generate counterfactuals with your recourse method and evaluate your recourse method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rocelib.recourse_methods.RecourseGenerator import RecourseGenerator\n",
    "import pandas as pd\n",
    "class MyRecourseMethod(RecourseGenerator):\n",
    "    def _generation_method(self, instance, column_name=\"target\", neg_value=0, **kwargs):\n",
    "        ce = pd.DataFrame([[1, 2, 3, 4]], columns=[\"new_recourse_method_1\", \"new_recourse_method_2\", \"new_recourse_method_3\", \"new_recourse_method_4\"])\n",
    "        return ce\n",
    "ct.add_recourse_method(\"my_recourse_method\", MyRecourseMethod)\n",
    "ces = ct.generate(['my_recourse_method'])\n",
    "print(ces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Your Own Evaluation Metric\n",
    "To add your own evaluation metric, create the class for your evaluation metric. Your class should extend the Evaluator class and implement the `evaluate` method.\n",
    "You should then call `ct.add_evaluation_metric(metric name, metric class)`\n",
    "You can now use the library to evaluate recourse methods with this metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rocelib.evaluations.robustness_evaluations.Evaluator import Evaluator\n",
    "class MyEvaluator(Evaluator):\n",
    "    def evaluate(self, recourse_method, **kwargs):\n",
    "        return -1\n",
    "\n",
    "ct.add_evaluation_metric(\"my_evaluator\", MyEvaluator)\n",
    "ces = ct.generate(['MCE'])\n",
    "evals = ct.evaluate(['MCE'], ['my_evaluator'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Your Own Robustness Evaluation Metric\n",
    "To add your own evaluation metric, create the class for your evaluation metric. Your class should extend the corresponding evaluator superclass based on the robustness notion:\n",
    "IC should extend InputChangesRobustnessEvaluator\n",
    "MC should extend ModelChangesRobustnessEvaluator\n",
    "NE should extend NoisyExecutionRobustnessEvaluator\n",
    "MM should extend ModelMultiplicityRobustnessEvaluator\n",
    " \n",
    "Your class should then implement the `evaluate_single_instance` method.\n",
    "You should then call `ct.add_evaluation_metric(metric name, metric class)`\n",
    "You can now use the library to evaluate recourse methods with this robustness metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rocelib.evaluations.robustness_evaluations.ModelChangesRobustnessEvaluator import ModelChangesRobustnessEvaluator\n",
    "\n",
    "class NewMCEvaluator(ModelChangesRobustnessEvaluator):\n",
    "    def evaluate_single_instance(self, instance, counterfactual, **kwargs):\n",
    "        return True\n",
    "\n",
    "\n",
    "ct.add_evaluation_metric(\"my_mc_evaluator\", MyEvaluator)\n",
    "ces = ct.generate(['MCE'])\n",
    "evals = ct.evaluate(['MCE'], ['my_mc_evaluator'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
