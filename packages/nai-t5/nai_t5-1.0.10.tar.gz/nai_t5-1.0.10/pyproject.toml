# delete old artifacts using:
# rm -rf dist/ build/ *.egg-info/

# build using 
# python -m build

# check artifact:
# twine check dist/*

# if it fails, try upgrading all this and building again:
# pip install --upgrade build hatchling twine importlib-metadata

# upload to test PyPI:
# python3 -m twine upload --repository testpypi dist/*

# clean build & publish to test PyPI:
# rm -rf dist/ build/ *.egg-info/ && python -m build && python3 -m twine upload --repository testpypi dist/*

# publish to main PyPI:
# python3 -m twine upload dist/*

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "nai-t5"
version = "1.0.10"
description = "NovelAI T5"
authors = [{name = "NovelAI/Anlatan Inc."}]
readme = "README.md"
license = {text = "Apache-2.0"}
requires-python = ">=3.10"
keywords = ["t5", "novelai"]
dependencies = [
    "torch",
    "einops",
    "pydantic",
]

[project.urls]
"Homepage" = "https://github.com/NovelAI/t5"
"Issue Tracker" = "https://github.com/NovelAI/t5/issues"

[project.optional-dependencies]
hf = ["transformers", "tokenizers"]
sp = ["sentencepiece"]
tensorizer = ["tensorizer"]
bench = ["tabulate", "triton"]

[project.scripts]
t5_serialize = "nai_t5.scripts.t5_serialize:main"
t5_serialize_dtensor = "nai_t5.scripts.t5_serialize_dtensor:main"
tokenizer_hf_to_sentencepiece = "nai_t5.scripts.tokenizer_hf_to_sentencepiece:main"

[tool.hatch.build.targets.sdist]
include = [
  "nai_t5/",
  "pyproject.toml",
  "README.md",
  "LICENSE",
  "requirements.txt",
]